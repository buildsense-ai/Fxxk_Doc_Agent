"""
é«˜è´¨é‡åµŒå…¥æœåŠ¡ (Advanced Embedding Service)
ä½¿ç”¨BGE-M3æ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼Œä¸“æ³¨äºæ–‡æœ¬æè¿°çš„è¯­ä¹‰ç†è§£
"""

import numpy as np
from typing import List, Optional, Dict, Any, Tuple
import logging
from pathlib import Path
import torch
import warnings

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class AdvancedEmbeddingService:
    """
    é«˜è´¨é‡åµŒå…¥æœåŠ¡
    ä½¿ç”¨BGE-M3æ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥ï¼Œä¸“æ³¨äºæè¿°æ–‡æœ¬çš„è¯­ä¹‰ç†è§£
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é«˜è´¨é‡åµŒå…¥æœåŠ¡"""
        self.text_model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.embedding_dim = 1024  # BGE-M3çš„embeddingç»´åº¦
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–é«˜è´¨é‡åµŒå…¥æœåŠ¡ (è®¾å¤‡: {self.device})")
        
        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
        self._init_text_model()
    
    def _init_text_model(self):
        """åˆå§‹åŒ–BGE-M3æ–‡æœ¬åµŒå…¥æ¨¡å‹"""
        try:
            from FlagEmbedding import BGEM3FlagModel
            
            logger.info("ğŸ“¦ åŠ è½½BGE-M3æ¨¡å‹...")
            self.text_model = BGEM3FlagModel(
                'BAAI/bge-m3',
                use_fp16=True,  # ä½¿ç”¨åŠç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜
                device=self.device
            )
            logger.info("âœ… BGE-M3æ–‡æœ¬åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except ImportError as e:
            logger.error(f"âŒ BGE-M3æ¨¡å‹ä¾èµ–æœªå®‰è£…: {e}")
            self.text_model = None
        except Exception as e:
            logger.error(f"âŒ BGE-M3æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.text_model = None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.text_model is not None
    
    def embed_texts(self, texts: List[str], instruction: str = "") -> np.ndarray:
        """
        å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡ŒåµŒå…¥
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            instruction: æŸ¥è¯¢æŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åµŒå…¥å‘é‡æ•°ç»„ (n_texts, embedding_dim)
        """
        if not self.is_available():
            logger.error("âŒ BGE-M3æ¨¡å‹ä¸å¯ç”¨")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            return np.zeros((len(texts), self.embedding_dim), dtype=np.float32)
        
        if not texts:
            logger.warning("âš ï¸ æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
            return np.zeros((0, self.embedding_dim), dtype=np.float32)
        
        try:
            # é¢å¤–çš„Noneæ£€æŸ¥ä»¥æ»¡è¶³ç±»å‹æ£€æŸ¥å™¨
            if self.text_model is None:
                logger.error("âŒ text_modelä¸ºNone")
                return np.zeros((len(texts), self.embedding_dim), dtype=np.float32)
            
            logger.info(f"ğŸ”¤ åµŒå…¥ {len(texts)} ä¸ªæ–‡æœ¬...")
            
            # å¦‚æœæœ‰æŸ¥è¯¢æŒ‡ä»¤ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬æ·»åŠ æŒ‡ä»¤å‰ç¼€
            if instruction:
                processed_texts = [f"{instruction} {text}" for text in texts]
            else:
                processed_texts = texts
            
            # ä½¿ç”¨BGE-M3è¿›è¡ŒåµŒå…¥
            embeddings = self.text_model.encode(
                processed_texts,
                batch_size=32,
                max_length=8192,  # BGE-M3æ”¯æŒçš„æœ€å¤§é•¿åº¦
                return_dense=True,  # åªè¿”å›dense embedding
                return_sparse=False,
                return_colbert_vecs=False
            )
            
            # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ ¼å¼
            if isinstance(embeddings, dict) and 'dense_vecs' in embeddings:
                embeddings = embeddings['dense_vecs']
            
            embeddings = np.array(embeddings, dtype=np.float32)
            
            logger.info(f"âœ… æ–‡æœ¬åµŒå…¥å®Œæˆ: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬åµŒå…¥å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            return np.zeros((len(texts), self.embedding_dim), dtype=np.float32)
    
    def embed_query(self, query: str) -> np.ndarray:
        """
        ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æŸ¥è¯¢å‘é‡ (embedding_dim,)
        """
        if not query.strip():
            logger.warning("âš ï¸ æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return np.zeros(self.embedding_dim, dtype=np.float32)
        
        # ä¸ºæŸ¥è¯¢æ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤ä»¥æå‡æ£€ç´¢æ•ˆæœ
        query_instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
        
        try:
            embeddings = self.embed_texts([query], instruction=query_instruction)
            return embeddings[0]  # è¿”å›å•ä¸ªå‘é‡
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥: {e}")
            return np.zeros(self.embedding_dim, dtype=np.float32)
    
    def embed_documents(self, documents: List[str]) -> np.ndarray:
        """
        ä¸ºæ–‡æ¡£åˆ—è¡¨ç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            documents: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æ–‡æ¡£åµŒå…¥çŸ©é˜µ (n_docs, embedding_dim)
        """
        if not documents:
            logger.warning("âš ï¸ æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return np.zeros((0, self.embedding_dim), dtype=np.float32)
        
        try:
            # å¯¹æ–‡æ¡£è¿›è¡ŒåµŒå…¥ï¼ˆä¸æ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤ï¼‰
            embeddings = self.embed_texts(documents)
            logger.info(f"âœ… æ–‡æ¡£åµŒå…¥å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£åµŒå…¥å¤±è´¥: {e}")
            return np.zeros((len(documents), self.embedding_dim), dtype=np.float32)
    
    def compute_similarity(self, query_embedding: np.ndarray, doc_embeddings: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡ (embedding_dim,)
            doc_embeddings: æ–‡æ¡£åµŒå…¥çŸ©é˜µ (n_docs, embedding_dim)
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„ (n_docs,)
        """
        try:
            # å½’ä¸€åŒ–å‘é‡
            query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-8)
            doc_norms = doc_embeddings / (np.linalg.norm(doc_embeddings, axis=1, keepdims=True) + 1e-8)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = np.dot(doc_norms, query_norm)
            
            return similarities
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return np.zeros(len(doc_embeddings), dtype=np.float32)
    
    def get_embedding_dimension(self) -> int:
        """è·å–åµŒå…¥å‘é‡ç»´åº¦"""
        return self.embedding_dim
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "service": "Advanced (BGE-M3 only)",
            "text_model": "BAAI/bge-m3" if self.text_model else None,
            "image_model": "None (å›¾ç‰‡åµŒå…¥å·²ç§»é™¤)",
            "embedding_dim": self.embedding_dim,
            "device": self.device,
            "available": self.is_available(),
            "features": [
                "å¤šè¯­è¨€æ”¯æŒ", 
                "é•¿æ–‡æœ¬å¤„ç†(8192 tokens)", 
                "é«˜è´¨é‡ä¸­æ–‡embedding",
                "ä»…æ–‡æœ¬æè¿°åµŒå…¥"
            ]
        }
    
    def test_embedding(self) -> Dict[str, Any]:
        """æµ‹è¯•åµŒå…¥åŠŸèƒ½"""
        if not self.is_available():
            return {
                "success": False,
                "error": "BGE-M3æ¨¡å‹ä¸å¯ç”¨",
                "details": {}
            }
        
        try:
            # æµ‹è¯•æ–‡æœ¬åµŒå…¥
            test_texts = [
                "è¿™æ˜¯ä¸€å¼ ç¾ä¸½çš„é£æ™¯ç…§ç‰‡",
                "å¤åº™æ ‡å¿—ç‰Œçš„æ–‡ç‰©ä¿æŠ¤æ ‡è¯†",
                "å’ªå’ªèººåœ¨åºŠä¸Šä¼‘æ¯"
            ]
            
            text_embeddings = self.embed_texts(test_texts)
            query_embedding = self.embed_query("å¤åº™æ ‡å¿—ç‰Œ")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = self.compute_similarity(query_embedding, text_embeddings)
            
            return {
                "success": True,
                "error": None,
                "details": {
                    "text_embedding_shape": text_embeddings.shape,
                    "query_embedding_shape": query_embedding.shape,
                    "similarities": similarities.tolist(),
                    "model_info": self.get_model_info()
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "details": {}
            } 