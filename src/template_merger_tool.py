#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡æ¿åˆå¹¶å·¥å…·ï¼šAIæ™ºèƒ½åˆå¹¶åŸå§‹æ–‡æ¡£ä¸æ¨¡æ¿ï¼Œæ”¯æŒæ¨¡æ¿æ’å…¥åŠŸèƒ½
"""

import os
import json
import logging
import traceback
import tempfile
import subprocess
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple
from pathlib import Path

from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from openai import OpenAI
import fitz  # PyMuPDF
from docx import Document as DocxDocument

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
    logger.info("âœ… å·²åŠ è½½.envç¯å¢ƒå˜é‡æ–‡ä»¶")
except ImportError:
    logger.warning("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œå°†ç›´æ¥ä»ç³»ç»Ÿç¯å¢ƒå˜é‡è¯»å–é…ç½®")
except Exception as e:
    logger.warning(f"âš ï¸ åŠ è½½.envæ–‡ä»¶æ—¶å‡ºç°é—®é¢˜: {e}")

def get_fill_data_prompt(template_structure: str, placeholders: str, input_data: str) -> str:
    """
    Generates a prompt for the AI to handle hybrid mapping: both template structure and placeholders.
    """
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ··åˆæ¨¡å¼æ–‡æ¡£å¡«å†™åŠ©æ‰‹ã€‚ä½ éœ€è¦åŒæ—¶å¤„ç†ä¸‰ç§ä»»åŠ¡ï¼š
1. **æ¨¡æ¿ç»“æ„åŒ¹é…**ï¼šå°†JSONæ•°æ®æ˜ å°„åˆ°Wordæ–‡æ¡£çš„è¡¨æ ¼å•å…ƒæ ¼å’Œæ®µè½ç»“æ„ä¸­
2. **å ä½ç¬¦åŒ¹é…**ï¼šä¸ºç‰¹å®šçš„å ä½ç¬¦æ‰¾åˆ°å¯¹åº”çš„æ•°æ®å€¼
3. **å›¾ç‰‡å¼•ç”¨å¤„ç†**ï¼šå¤„ç†å›¾ç‰‡å ä½ç¬¦ï¼Œç¡®ä¿å›¾ç‰‡èƒ½æ­£ç¡®å¼•ç”¨

---

**æ ¸å¿ƒä»»åŠ¡ï¼šç”Ÿæˆç»Ÿä¸€çš„å¡«å……æ•°æ®æ˜ å°„**

ä½ å°†è·å¾—ï¼š
- **æ¨¡æ¿ç»“æ„**ï¼šæ–‡æ¡£ä¸­æ‰€æœ‰å•å…ƒæ ¼å’Œæ®µè½çš„ç»“æ„åŒ–è¡¨ç¤º
- **å ä½ç¬¦åˆ—è¡¨**ï¼šä»ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚"é¡¹ç›®åç§°ï¼š"å’Œ"è‡´____ï¼ˆç›‘ç†å•ä½ï¼‰"ï¼‰æå–çš„å ä½ç¬¦
- **è¾“å…¥æ•°æ®**ï¼šéœ€è¦æ˜ å°„çš„JSONæ•°æ®ï¼ˆå¯èƒ½åŒ…å«attachments_mapå›¾ç‰‡ä¿¡æ¯ï¼‰

ä½ çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä¸‰ç§ç±»å‹çš„æ˜ å°„ï¼š
- **ç»“æ„æ˜ å°„**ï¼šé”®å¦‚"table_0_row_1_col_1"æˆ–"paragraph_3"ï¼Œç”¨äºå¡«å……æ¨¡æ¿ç»“æ„
- **å ä½ç¬¦æ˜ å°„**ï¼šé”®å¦‚"label_é¡¹ç›®åç§°"æˆ–"inline_ç›‘ç†å•ä½"ï¼Œç”¨äºæ›¿æ¢å ä½ç¬¦
- **å›¾ç‰‡æ˜ å°„**ï¼šå¦‚æœè¾“å…¥æ•°æ®åŒ…å«attachments_mapï¼Œç›´æ¥å°†å…¶åŒ…å«åœ¨è¾“å‡ºä¸­

---

**é‡è¦åŸåˆ™ï¼šåªå¡«å……ç¡®å®šçš„æ•°æ®**

âš ï¸ **å…³é”®è¦æ±‚**ï¼š
- **åªæœ‰åœ¨è¾“å…¥æ•°æ®ä¸­æ‰¾åˆ°æ˜ç¡®å¯¹åº”çš„ä¿¡æ¯æ—¶ï¼Œæ‰è¾“å‡ºè¯¥é”®å€¼å¯¹**
- **å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„æ•°æ®ï¼Œè¯·å®Œå…¨çœç•¥è¯¥é”®ï¼Œä¸è¦è¾“å‡ºä»»ä½•å ä½æ–‡æœ¬å¦‚"å¾…å¡«å†™"ã€"____"ç­‰**
- **å®å¯ç•™ç©ºï¼Œä¹Ÿä¸è¦å¡«å…¥ä¸ç¡®å®šçš„å†…å®¹**

ç°åœ¨è¯·æ ¹æ®ä¸‹æ–¹çš„æ•°æ®è¿›è¡Œæ··åˆæ˜ å°„ï¼š

**æ¨¡æ¿ç»“æ„:**
```json
{template_structure}
```

**å ä½ç¬¦åˆ—è¡¨:**
```json
{placeholders}
```

**è¾“å…¥æ•°æ®:**
```json
{input_data}
```

**é‡è¦è¦æ±‚:**
- **æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡**
- è¾“å‡ºç»Ÿä¸€çš„JSONå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰ç±»å‹çš„æ˜ å°„
- å¯¹å ä½ç¬¦è¿›è¡Œæ™ºèƒ½è¯­ä¹‰åŒ¹é…
- ä¿æŒåŸæœ‰çš„æ¨¡æ¿ç»“æ„å¡«å……é€»è¾‘
- **å¦‚æœè¾“å…¥æ•°æ®åŒ…å«attachments_mapï¼Œå¿…é¡»å®Œæ•´åŒ…å«åœ¨è¾“å‡ºä¸­**
- **é‡è¦ï¼šå¦‚æœæ‰¾ä¸åˆ°å¯¹åº”æ•°æ®ï¼Œå®Œå…¨çœç•¥è¯¥é”®ï¼Œä¸è¦å¡«å…¥"å¾…å¡«å†™"ç­‰å ä½æ–‡æœ¬**
- åªè¾“å‡ºæœ€ç»ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«è§£é‡Šè¯´æ˜æˆ–Markdownæ ¼å¼
"""

class ProcessingError(Exception):
    """è‡ªå®šä¹‰å¤„ç†å¼‚å¸¸"""
    def __init__(self, message: str, error_code: str, status_code: int = 500):
        self.message = message
        self.error_code = error_code
        self.status_code = status_code
        super().__init__(self.message)

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨"""
    
    def __init__(self):
        logger.info("ğŸ“„ æ–‡æ¡£æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_from_file_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–å†…å®¹"""
        if not os.path.exists(file_path):
            raise ProcessingError(
                f"åŸå§‹æ–‡æ¡£ä¸å­˜åœ¨: {file_path}",
                "FILE_NOT_FOUND",
                404
            )
        return self._extract_content(file_path)
    
    def convert_doc_to_docx(self, doc_path: str) -> str:
        """ä½¿ç”¨LibreOfficeå°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ–‡ä»¶"""
        logger.info("ğŸ”„ å¼€å§‹DOCåˆ°DOCXè½¬æ¢...")
        
        if not os.path.exists(doc_path):
            logger.error(f"âŒ DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
            raise ProcessingError(f"DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}", "FILE_NOT_FOUND", 404)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        docx_path = doc_path.replace('.doc', '_converted.docx')
        
        try:
            # æ£€æŸ¥LibreOfficeæ˜¯å¦å¯ç”¨
            logger.info("ğŸ” æ£€æŸ¥LibreOfficeå¯ç”¨æ€§...")
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„LibreOfficeè·¯å¾„
            libreoffice_paths = [
                r'C:\Program Files\LibreOffice\program\soffice.exe',  # Windows 64ä½
                r'C:\Program Files (x86)\LibreOffice\program\soffice.exe',  # Windows 32ä½
                '/Applications/LibreOffice.app/Contents/MacOS/soffice',  # macOS
                'libreoffice',  # Linux/Windows PATH
                'soffice',  # å¤‡ç”¨å‘½ä»¤
            ]
            
            libreoffice_cmd = None
            for path in libreoffice_paths:
                try:
                    result = subprocess.run([path, '--version'], 
                                          capture_output=True, 
                                          text=True, 
                                          timeout=10)
                    if result.returncode == 0:
                        libreoffice_cmd = path
                        logger.info(f"âœ… æ‰¾åˆ°LibreOffice: {path}")
                        break
                except (FileNotFoundError, subprocess.TimeoutExpired):
                    continue
            
            if not libreoffice_cmd:
                logger.error("âŒ æœªæ‰¾åˆ°LibreOfficeï¼Œè¯·ç¡®ä¿å·²å®‰è£…LibreOffice")
                raise ProcessingError("LibreOfficeæœªå®‰è£…æˆ–ä¸å¯ç”¨", "LIBREOFFICE_NOT_FOUND", 500)
            
            # æ‰§è¡Œè½¬æ¢
            logger.info(f"ğŸ“„ æ­£åœ¨è½¬æ¢: {doc_path} -> {docx_path}")
            
            # åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
            if os.path.exists(docx_path):
                os.remove(docx_path)
                logger.info("ğŸ—‘ï¸ åˆ é™¤å·²å­˜åœ¨çš„è½¬æ¢æ–‡ä»¶")
            
            # LibreOfficeè½¬æ¢å‘½ä»¤
            cmd = [
                libreoffice_cmd,
                '--headless',
                '--convert-to', 'docx',
                '--outdir', os.path.dirname(doc_path),
                doc_path
            ]
            
            logger.info(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=30)
            
            if result.returncode != 0:
                logger.error(f"âŒ LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                raise ProcessingError(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}", "LIBREOFFICE_CONVERSION_FAILED", 500)
            
            # æ£€æŸ¥è½¬æ¢åçš„æ–‡ä»¶
            expected_docx = doc_path.replace('.doc', '.docx')
            if os.path.exists(expected_docx):
                # é‡å‘½åä¸ºæˆ‘ä»¬æœŸæœ›çš„æ–‡ä»¶å
                if expected_docx != docx_path:
                    os.rename(expected_docx, docx_path)
                
                logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {docx_path}")
                return docx_path
            else:
                logger.error(f"âŒ è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°: {expected_docx}")
                raise ProcessingError("è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°", "CONVERSION_OUTPUT_NOT_FOUND", 500)
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ LibreOfficeè½¬æ¢è¶…æ—¶")
            raise ProcessingError("LibreOfficeè½¬æ¢è¶…æ—¶", "LIBREOFFICE_TIMEOUT", 500)
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise ProcessingError(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", "CONVERSION_ERROR", 500)
    
    def _extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹çš„æ ¸å¿ƒæ–¹æ³•"""
        logger.info(f"ğŸ“„ å¼€å§‹æå–æ–‡æ¡£å†…å®¹: {Path(file_path).name}")
        
        content = ""
        converted_file = None
        
        try:
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.doc':
                # Convert .doc to .docx using LibreOffice
                logger.info("ğŸ”„ æ£€æµ‹åˆ°.docæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢ä¸º.docx...")
                converted_file = self.convert_doc_to_docx(file_path)
                file_path = converted_file
                file_ext = '.docx'
            
            if file_ext == '.docx':
                doc = DocxDocument(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
                
                # æå–è¡¨æ ¼å†…å®¹
                for table in doc.tables:
                    for row in table.rows:
                        row_text = " | ".join([cell.text.strip() for cell in row.cells])
                        if row_text.strip():
                            content += f"\nè¡¨æ ¼è¡Œ: {row_text}"
            
            elif file_ext == '.pdf':
                doc = fitz.open(file_path)
                for page in doc:
                    content += page.get_text()
                doc.close()
            
            elif file_ext in ['.txt', '.md']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            else:
                raise ProcessingError(
                    f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}",
                    "UNSUPPORTED_FORMAT",
                    422
                )
            
            if not content.strip():
                raise ProcessingError(
                    "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                    "EMPTY_DOCUMENT",
                    422
                )
            
            logger.info(f"âœ… æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # Cleanup converted file if it exists
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶: {converted_file}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶å¤±è´¥: {e}")
            
            return content.strip()
            
        except ProcessingError:
            # Cleanup converted file on error
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                except:
                    pass
            raise
        except Exception as e:
            # Cleanup converted file on error
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                except:
                    pass
            logger.error(f"âŒ æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            raise ProcessingError(
                f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}",
                "EXTRACTION_ERROR",
                500
            )

class ContentMerger:
    """å†…å®¹æ™ºèƒ½åˆå¹¶å™¨"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.model = "google/gemini-2.5-pro-preview"
        logger.info("ğŸ§  å†…å®¹åˆå¹¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """ä½¿ç”¨AIæ™ºèƒ½åˆå¹¶æ¨¡æ¿JSONå’ŒåŸå§‹å†…å®¹"""
        logger.info("ğŸ§  å¼€å§‹AIæ™ºèƒ½åˆå¹¶...")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
        test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
        if test_mode or self.client.api_key == "test-api-key-for-testing":
            logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAIåˆå¹¶")
            return self._mock_merge_content(template_json, original_content)
        
        # ä½¿ç”¨ä¸“ä¸š prompt
        template_structure = json.dumps(template_json, ensure_ascii=False, indent=2)
        placeholders = json.dumps(list(template_json.keys()), ensure_ascii=False, indent=2)
        input_data = original_content
        
        prompt = get_fill_data_prompt(template_structure, placeholders, input_data)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            
            if not response or not response.choices or not response.choices[0].message.content:
                raise ProcessingError(
                    "AIå“åº”æ— æ•ˆæˆ–ä¸ºç©º",
                    "AI_NO_RESPONSE",
                    500
                )
            
            # æå–JSONå†…å®¹
            response_content = response.choices[0].message.content.strip()
            json_str = self._extract_json_from_response(response_content)
            
            try:
                merged_content = json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                logger.error(f"AIå“åº”å†…å®¹: {response_content}")
                raise ProcessingError(
                    f"AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(e)}",
                    "AI_INVALID_JSON",
                    422
                )
            
            # éªŒè¯åˆå¹¶ç»“æœ
            if not isinstance(merged_content, dict):
                raise ProcessingError(
                    "AIè¿”å›çš„å†…å®¹ä¸æ˜¯å­—å…¸æ ¼å¼",
                    "AI_INVALID_FORMAT",
                    422
                )
            
            logger.info(f"âœ… AIåˆå¹¶æˆåŠŸï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
            return merged_content
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ AIåˆå¹¶å¤±è´¥: {e}")
            raise ProcessingError(
                f"AIåˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "AI_MERGE_ERROR",
                500
            )
    
    def _mock_merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """æ¨¡æ‹ŸAIåˆå¹¶ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰"""
        logger.info("ğŸ§ª æ¨¡æ‹ŸAIåˆå¹¶æ¨¡å¼")
        
        merged_content = {}
        content_lines = original_content.split('\n')
        content_preview = ' '.join(content_lines[:5])[:200]
        
        for key, description in template_json.items():
            merged_content[key] = f"""æ ¹æ®åŸå§‹æ–‡æ¡£å†…å®¹ç”Ÿæˆçš„{key}ç« èŠ‚ï¼š

{description}

åŸºäºåŸå§‹æ–‡æ¡£çš„ç›¸å…³ä¿¡æ¯ï¼š
{content_preview}

æœ¬ç« èŠ‚å†…å®¹å·²æ ¹æ®æ¨¡æ¿è¦æ±‚è¿›è¡Œæ™ºèƒ½æ•´åˆï¼Œç¡®ä¿ç¬¦åˆå·¥ç¨‹æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼å’Œè¦æ±‚ã€‚"""
        
        logger.info(f"âœ… æ¨¡æ‹Ÿåˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
        return merged_content
    
    def _extract_json_from_response(self, response_content: str) -> str:
        """ä»AIå“åº”ä¸­æå–JSONå†…å®¹"""
        # å°è¯•æå–JSON
        if "```json" in response_content:
            start = response_content.find("```json") + 7
            end = response_content.find("```", start)
            if end != -1:
                return response_content[start:end].strip()
            else:
                return response_content[response_content.find("```json") + 7:].strip()
        elif response_content.startswith("{") and response_content.endswith("}"):
            return response_content
        else:
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start_idx = response_content.find("{")
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(response_content[start_idx:], start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            return response_content[start_idx:i+1]
            return response_content

class TemplateMergerTool:
    """æ¨¡æ¿åˆå¹¶å·¥å…· - AIæ™ºèƒ½åˆå¹¶åŸå§‹æ–‡æ¡£ä¸æ¨¡æ¿"""
    
    def __init__(self, deepseek_client=None):
        """åˆå§‹åŒ–å·¥å…·"""
        self.name = "template_merger"
        self.description = """ğŸ“‹ æ¨¡æ¿åˆå¹¶å·¥å…· - AIæ™ºèƒ½åˆå¹¶åŸå§‹æ–‡æ¡£ä¸æ¨¡æ¿JSON

ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ğŸ“„ æ™ºèƒ½å†…å®¹æå–ï¼šæ”¯æŒDOC/DOCX/PDF/TXTæ ¼å¼æ–‡æ¡£
- ğŸ§  AIæ™ºèƒ½åˆå¹¶ï¼šå°†åŸå§‹æ–‡æ¡£å†…å®¹ä¸æ¨¡æ¿JSONæ™ºèƒ½åˆå¹¶
- ğŸ“ ç»“æ„åŒ–ç”Ÿæˆï¼šç”Ÿæˆç¬¦åˆæ¨¡æ¿è¦æ±‚çš„ç»“æ„åŒ–æ–‡æ¡£
- ğŸ”„ æ ¼å¼è½¬æ¢ï¼šè‡ªåŠ¨å¤„ç†DOCåˆ°DOCXçš„è½¬æ¢

ğŸ’¡ **é€‚ç”¨åœºæ™¯ï¼š**
- åŸºäºæ¨¡æ¿ç”Ÿæˆæ ‡å‡†åŒ–æ–‡æ¡£
- å°†éç»“æ„åŒ–å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æ¡£
- æ‰¹é‡æ–‡æ¡£å¤„ç†å’Œæ ‡å‡†åŒ–
- å†…å®¹æ™ºèƒ½æ•´åˆå’Œé‡ç»„

âš™ï¸ **ä½¿ç”¨æ–¹æ³•ï¼š**
Action Input: {"action": "merge", "template_json": {"ç« èŠ‚1": "æè¿°"}, "content_source": "æ–‡ä»¶è·¯å¾„æˆ–å†…å®¹æ–‡æœ¬"}"""
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir = "generated_docs"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            
        logger.info("ğŸ“‹ æ¨¡æ¿åˆå¹¶å·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def get_tool_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "action": {
                    "type": "string",
                    "description": "æ“ä½œç±»å‹",
                    "enum": ["merge", "extract", "analyze"]
                },
                "template_json": {
                    "type": "object",
                    "description": "æ¨¡æ¿JSONç»“æ„ï¼Œé”®ä¸ºç« èŠ‚åï¼Œå€¼ä¸ºç« èŠ‚æè¿°"
                },
                "content_source": {
                    "type": "string",
                    "description": "å†…å®¹æºï¼šå¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„æˆ–ç›´æ¥çš„æ–‡æœ¬å†…å®¹"
                },
                "output_path": {
                    "type": "string",
                    "description": "å¯é€‰çš„è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ"
                }
            },
            "required": ["action"]
        }
    
    def execute(self, **kwargs) -> str:
        """æ‰§è¡Œå·¥å…·æ“ä½œ"""
        action = kwargs.get("action", "").lower()
        
        try:
            if action == "merge":
                return self._merge_template(kwargs)
            elif action == "extract":
                return self._extract_content(kwargs)
            elif action == "analyze":
                return self._analyze_template(kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}ã€‚æ”¯æŒçš„æ“ä½œ: merge, extract, analyze"
                
        except Exception as e:
            error_msg = f"âŒ æ¨¡æ¿åˆå¹¶å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _get_api_key(self) -> str:
        """è·å–OpenRouter APIå¯†é’¥"""
        api_key = os.environ.get("OPENROUTER_API_KEY")
        if not api_key:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
            test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
            if test_mode:
                logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAPIå¯†é’¥")
                return "test-api-key-for-testing"
            
            logger.error("âŒ æœªæ‰¾åˆ°OPENROUTER_API_KEY")
            raise RuntimeError("ç¼ºå°‘å¿…éœ€çš„APIå¯†é’¥é…ç½®")
        return api_key
    
    def _merge_template(self, params: Dict[str, Any]) -> str:
        """æ‰§è¡Œæ¨¡æ¿åˆå¹¶æ“ä½œ"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡æ¿åˆå¹¶æ“ä½œ...")
        
        # è·å–å‚æ•°
        template_json = params.get("template_json")
        content_source = params.get("content_source")
        output_path = params.get("output_path")
        
        if not template_json:
            return "âŒ ç¼ºå°‘template_jsonå‚æ•°"
        
        if not content_source:
            return "âŒ ç¼ºå°‘content_sourceå‚æ•°"
        
        try:
            # è·å–APIå¯†é’¥
            api_key = self._get_api_key()
            
            # æå–å†…å®¹
            if os.path.exists(content_source):
                # ä»æ–‡ä»¶æå–å†…å®¹
                extractor = DocumentExtractor()
                original_content = extractor.extract_from_file_path(content_source)
                logger.info("âœ… ä»æ–‡ä»¶æå–å†…å®¹æˆåŠŸ")
            else:
                # ç›´æ¥ä½¿ç”¨æä¾›çš„å†…å®¹
                original_content = content_source
                logger.info("âœ… ä½¿ç”¨ç›´æ¥æä¾›çš„å†…å®¹")
            
            # æ‰§è¡ŒAIåˆå¹¶
            merger = ContentMerger(api_key)
            merged_content = merger.merge_content(template_json, original_content)
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            if not output_path:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_filename = f"merged_document_{timestamp}.docx"
                output_path = os.path.join(self.output_dir, output_filename)
            
            # ç”ŸæˆDOCXæ–‡æ¡£
            doc = Document()
            doc.add_heading('AIæ™ºèƒ½åˆå¹¶æ–‡æ¡£', 0)
            
            for section_title, section_content in merged_content.items():
                doc.add_heading(str(section_title), level=1)
                doc.add_paragraph(str(section_content))
            
            doc.save(output_path)
            
            # åˆ†æç¼ºå¤±å­—æ®µ
            missing_fields = []
            for key, value in merged_content.items():
                if not value or len(str(value).strip()) < 10:
                    missing_fields.append(key)
                elif "å¾…å¡«å†™" in str(value) or "éœ€è¦è¡¥å……" in str(value) or "____" in str(value):
                    missing_fields.append(key)
            
            # ç”Ÿæˆç»“æœæŠ¥å‘Š
            completion_rate = (len(template_json) - len(missing_fields)) / len(template_json) if template_json else 0
            
            result = f"""âœ… æ¨¡æ¿åˆå¹¶å®Œæˆï¼

ğŸ“„ **ç”Ÿæˆæ–‡æ¡£**: {output_path}
ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯**:
   - æ¨¡æ¿ç« èŠ‚: {len(template_json)} ä¸ª
   - å·²å¡«å……ç« èŠ‚: {len(template_json) - len(missing_fields)} ä¸ª
   - å®Œæˆç‡: {completion_rate:.1%}
   - æ–‡æ¡£å¤§å°: {os.path.getsize(output_path) if os.path.exists(output_path) else 0} å­—èŠ‚

ğŸ“‹ **ç« èŠ‚è¯¦æƒ…**:"""
            
            for key, value in merged_content.items():
                status = "âœ…" if key not in missing_fields else "âš ï¸"
                preview = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
                result += f"\n   {status} {key}: {preview}"
            
            if missing_fields:
                result += f"\n\nâš ï¸ **éœ€è¦è¡¥å……çš„ç« èŠ‚**: {', '.join(missing_fields)}"
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿åˆå¹¶å¤±è´¥: {e}")
            return f"âŒ æ¨¡æ¿åˆå¹¶å¤±è´¥: {str(e)}"
    
    def _extract_content(self, params: Dict[str, Any]) -> str:
        """æ‰§è¡Œå†…å®¹æå–æ“ä½œ"""
        logger.info("ğŸ“„ å¼€å§‹å†…å®¹æå–æ“ä½œ...")
        
        content_source = params.get("content_source")
        if not content_source:
            return "âŒ ç¼ºå°‘content_sourceå‚æ•°"
        
        if not os.path.exists(content_source):
            return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {content_source}"
        
        try:
            extractor = DocumentExtractor()
            content = extractor.extract_from_file_path(content_source)
            
            result = f"""âœ… å†…å®¹æå–å®Œæˆï¼

ğŸ“„ **æºæ–‡ä»¶**: {content_source}
ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯**:
   - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦
   - æ–‡ä»¶å¤§å°: {os.path.getsize(content_source)} å­—èŠ‚

ğŸ“ **å†…å®¹é¢„è§ˆ**:
{content[:500]}{'...' if len(content) > 500 else ''}"""
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
            return f"âŒ å†…å®¹æå–å¤±è´¥: {str(e)}"
    
    def _analyze_template(self, params: Dict[str, Any]) -> str:
        """åˆ†ææ¨¡æ¿ç»“æ„"""
        logger.info("ğŸ” å¼€å§‹æ¨¡æ¿åˆ†ææ“ä½œ...")
        
        template_json = params.get("template_json")
        if not template_json:
            return "âŒ ç¼ºå°‘template_jsonå‚æ•°"
        
        try:
            result = f"""ğŸ“‹ æ¨¡æ¿ç»“æ„åˆ†æç»“æœï¼š

ğŸ“Š **åŸºæœ¬ç»Ÿè®¡**:
   - ç« èŠ‚æ€»æ•°: {len(template_json)} ä¸ª
   - æ¨¡æ¿ç±»å‹: {"å¤æ‚æ¨¡æ¿" if len(template_json) > 5 else "ç®€å•æ¨¡æ¿"}

ğŸ“ **ç« èŠ‚è¯¦æƒ…**:"""
            
            for i, (key, value) in enumerate(template_json.items(), 1):
                description_length = len(str(value))
                complexity = "è¯¦ç»†" if description_length > 100 else "ç®€å•" if description_length > 20 else "åŸºç¡€"
                result += f"\n   {i}. **{key}** ({complexity}): {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}"
            
            # æä¾›ä½¿ç”¨å»ºè®®
            result += f"\n\nğŸ’¡ **ä½¿ç”¨å»ºè®®**:"
            if len(template_json) > 10:
                result += "\n   - æ¨¡æ¿è¾ƒå¤æ‚ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†"
            if any(len(str(v)) < 10 for v in template_json.values()):
                result += "\n   - éƒ¨åˆ†ç« èŠ‚æè¿°è¾ƒç®€å•ï¼Œå¯èƒ½éœ€è¦æ›´è¯¦ç»†çš„è¯´æ˜"
            
            result += "\n   - å‡†å¤‡å¥½åŸå§‹å†…å®¹åï¼Œä½¿ç”¨mergeæ“ä½œè¿›è¡Œæ™ºèƒ½åˆå¹¶"
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿åˆ†æå¤±è´¥: {e}")
            return f"âŒ æ¨¡æ¿åˆ†æå¤±è´¥: {str(e)}" 