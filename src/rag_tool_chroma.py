"""
ChromaDB RAGå·¥å…· - é‡æ–°è®¾è®¡ç‰ˆ
æ ¸å¿ƒåŠŸèƒ½ï¼šæ–‡æ¡£embeddingå¤„ç†ã€æ™ºèƒ½æœç´¢ã€åŸºäºæ¨¡æ¿å­—æ®µçš„å†…å®¹å¡«å……
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import hashlib

try:
    import chromadb
    from chromadb.config import Settings
    import fitz  # PyMuPDF for PDF
    from docx import Document as DocxDocument
except ImportError as e:
    print(f"è­¦å‘Š: RAGå·¥å…·ä¾èµ–æœªå®‰è£…: {e}")
    print("è¯·å®‰è£…: pip install chromadb PyMuPDF python-docx")

try:
    from .tools import Tool
except ImportError:
    from tools import Tool

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨"""
    
    def extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        try:
            if file_ext == '.pdf':
                return self._extract_from_pdf(file_path)
            elif file_ext == '.docx':
                return self._extract_from_docx(file_path)
            elif file_ext in ['.txt', '.md']:
                return self._extract_from_text(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        except Exception as e:
            raise RuntimeError(f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}")
    
    def _extract_from_pdf(self, file_path: str) -> str:
        """ä»PDFæå–å†…å®¹"""
        content = ""
        doc = fitz.open(file_path)
        for page in doc:
            content += page.get_text()
        doc.close()
        return content.strip()
    
    def _extract_from_docx(self, file_path: str) -> str:
        """ä»DOCXæå–å†…å®¹"""
        doc = DocxDocument(file_path)
        content = []
        
        # æå–æ®µè½å†…å®¹
        for para in doc.paragraphs:
            if para.text.strip():
                content.append(para.text.strip())
        
        # æå–è¡¨æ ¼å†…å®¹
        for table in doc.tables:
            for row in table.rows:
                row_text = " | ".join([cell.text.strip() for cell in row.cells])
                if row_text.strip():
                    content.append(f"è¡¨æ ¼è¡Œ: {row_text}")
        
        return "\n".join(content)
    
    def _extract_from_text(self, file_path: str) -> str:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read().strip()

class ChromaVectorStore:
    """ChromaDBå‘é‡å­˜å‚¨"""
    
    def __init__(self, storage_dir: str):
        self.storage_dir = storage_dir
        self.client = chromadb.PersistentClient(
            path=storage_dir,
            settings=Settings(allow_reset=True, anonymized_telemetry=False)
        )
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
        # å°†é•¿æ–‡æ¡£åˆ†å—
        chunks = self._split_content(content)
        
        ids = []
        documents = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            chunk_id = f"{doc_id}_chunk_{i}"
            ids.append(chunk_id)
            documents.append(chunk)
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks)
            })
            metadatas.append(chunk_metadata)
        
        self.collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
        
        return len(chunks)
    
    def search_documents(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        formatted_results = []
        if results['documents'] and len(results['documents']) > 0:
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i]
                })
        
        return formatted_results
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯"""
        try:
            results = self.collection.get()
            documents = []
            
            # æŒ‰æ–‡æ¡£IDåˆ†ç»„
            doc_groups = {}
            for i, doc_id in enumerate(results['ids']):
                base_id = doc_id.split('_chunk_')[0]
                if base_id not in doc_groups:
                    doc_groups[base_id] = {
                        'id': base_id,
                        'metadata': results['metadatas'][i],
                        'chunks': 0
                    }
                doc_groups[base_id]['chunks'] += 1
            
            return list(doc_groups.values())
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        self.client.delete_collection("documents")
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
    
    def _split_content(self, content: str, chunk_size: int = 1000) -> List[str]:
        """å°†å†…å®¹åˆ†å—"""
        if len(content) <= chunk_size:
            return [content]
        
        chunks = []
        sentences = content.split('ã€‚')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence + 'ã€‚') <= chunk_size:
                current_chunk += sentence + 'ã€‚'
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + 'ã€‚'
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [content]

class TemplateFieldProcessor:
    """æ¨¡æ¿å­—æ®µå¤„ç†å™¨ - æ ¸å¿ƒæ™ºèƒ½å¡«å……åŠŸèƒ½"""
    
    def __init__(self, deepseek_client=None):
        self.deepseek_client = deepseek_client
    
    def fill_template_fields(self, template_fields_json: Dict[str, str], 
                           vector_store: ChromaVectorStore) -> Dict[str, str]:
        """
        åŸºäºæ¨¡æ¿å­—æ®µJSONè¿›è¡Œæ™ºèƒ½æœç´¢å’Œå†…å®¹å¡«å……
        
        Args:
            template_fields_json: æ¨¡æ¿å­—æ®µJSONï¼Œæ ¼å¼ä¸º {"å­—æ®µå": "å­—æ®µæè¿°æˆ–è¦æ±‚"}
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹
            
        Returns:
            å¡«å……å¥½çš„å­—æ®µJSONï¼Œæ ¼å¼ä¸º {"å­—æ®µå": "å¡«å……çš„å…·ä½“å†…å®¹"}
        """
        logger.info(f"ğŸ” å¼€å§‹åŸºäºæ¨¡æ¿å­—æ®µè¿›è¡Œæ™ºèƒ½å¡«å……ï¼Œå…± {len(template_fields_json)} ä¸ªå­—æ®µ")
        
        filled_fields = {}
        
        for field_name, field_requirement in template_fields_json.items():
            logger.info(f"ğŸ“ å¤„ç†å­—æ®µ: {field_name}")
            
            # 1. åŸºäºå­—æ®µè¦æ±‚æœç´¢ç›¸å…³å†…å®¹
            search_results = vector_store.search_documents(
                query=f"{field_name} {field_requirement}",
                n_results=3
            )
            
            # 2. æå–æœç´¢åˆ°çš„å†…å®¹
            relevant_content = []
            for result in search_results:
                relevant_content.append(result['content'])
            
            # 3. ä½¿ç”¨AIç”Ÿæˆå­—æ®µå†…å®¹ï¼ˆå¦‚æœæœ‰AIå®¢æˆ·ç«¯ï¼‰
            if self.deepseek_client and relevant_content:
                filled_content = self._generate_field_content_with_ai(
                    field_name, field_requirement, relevant_content
                )
            else:
                # åŸºç¡€æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æœç´¢åˆ°çš„æœ€ç›¸å…³å†…å®¹
                filled_content = self._generate_field_content_basic(
                    field_name, field_requirement, relevant_content
                )
            
            filled_fields[field_name] = filled_content
            logger.info(f"âœ… å­—æ®µ {field_name} å¡«å……å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(filled_content)} å­—ç¬¦")
        
        logger.info(f"ğŸ‰ æ‰€æœ‰å­—æ®µå¡«å……å®Œæˆï¼")
        return filled_fields
    
    def _generate_field_content_with_ai(self, field_name: str, field_requirement: str, 
                                       relevant_content: List[str]) -> str:
        """ä½¿ç”¨AIç”Ÿæˆå­—æ®µå†…å®¹"""
        content_text = "\n".join(relevant_content)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å¤„ç†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ä¸ºå­—æ®µç”Ÿæˆåˆé€‚çš„å†…å®¹ï¼š

å­—æ®µåç§°ï¼š{field_name}
å­—æ®µè¦æ±‚ï¼š{field_requirement}

ç›¸å…³èµ„æ–™å†…å®¹ï¼š
{content_text}

ä»»åŠ¡è¦æ±‚ï¼š
1. åŸºäºç›¸å…³èµ„æ–™å†…å®¹ï¼Œä¸ºè¯¥å­—æ®µç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„å†…å®¹
2. å†…å®¹åº”è¯¥ç¬¦åˆå­—æ®µè¦æ±‚å’Œæè¿°
3. ä¿æŒå†…å®¹çš„ä¸“ä¸šæ€§å’Œå®Œæ•´æ€§
4. å¦‚æœèµ„æ–™å†…å®¹ä¸è¶³ï¼Œè¯·åŸºäºå­—æ®µè¦æ±‚è¿›è¡Œåˆç†è¡¥å……
5. å†…å®¹é•¿åº¦é€‚ä¸­ï¼Œé‡ç‚¹çªå‡º

è¯·ç›´æ¥è¿”å›è¯¥å­—æ®µçš„å…·ä½“å†…å®¹ï¼Œä¸è¦åŒ…å«è§£é‡Šæ–‡å­—ã€‚
"""
        
        try:
            response = self.deepseek_client.chat([{"role": "user", "content": prompt}])
            return response.strip() if response else self._generate_field_content_basic(
                field_name, field_requirement, relevant_content
            )
        except Exception as e:
            logger.warning(f"AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼: {e}")
            return self._generate_field_content_basic(
                field_name, field_requirement, relevant_content
            )
    
    def _generate_field_content_basic(self, field_name: str, field_requirement: str, 
                                     relevant_content: List[str]) -> str:
        """åŸºç¡€æ¨¡å¼ç”Ÿæˆå­—æ®µå†…å®¹"""
        if not relevant_content:
            return f"[{field_name}]ï¼š{field_requirement}ï¼ˆå¾…è¡¥å……å…·ä½“å†…å®¹ï¼‰"
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹ä½œä¸ºåŸºç¡€
        base_content = relevant_content[0]
        
        # ç®€å•çš„å†…å®¹å¤„ç†
        if len(base_content) > 200:
            # æˆªå–å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
            summary = base_content[:200] + "..."
            return f"{summary}\n\nï¼ˆåŸºäºç›¸å…³èµ„æ–™æ•´ç†ï¼Œå¦‚éœ€è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒåŸå§‹æ–‡æ¡£ï¼‰"
        else:
            return base_content

class RAGTool(Tool):
    """é‡æ–°è®¾è®¡çš„RAGå·¥å…· - æ ¸å¿ƒä¸‰åŠŸèƒ½"""
    
    def __init__(self, storage_dir: str = "rag_storage", deepseek_client=None):
        super().__init__(
            name="rag_tool",
            description="æ–‡æ¡£å¤„ç†å’Œæ™ºèƒ½æ£€ç´¢å·¥å…·ã€‚æ”¯æŒæ–‡æ¡£embeddingå¤„ç†ã€åŸºäºæ¨¡æ¿å­—æ®µçš„æ™ºèƒ½æœç´¢å’Œå†…å®¹å¡«å……ã€‚"
        )
        
        self.storage_dir = storage_dir
        os.makedirs(storage_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.extractor = DocumentExtractor()
        self.vector_store = ChromaVectorStore(storage_dir)
        self.field_processor = TemplateFieldProcessor(deepseek_client)
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ"""
        try:
            if action == "upload":
                return self._upload_document(**kwargs)
            elif action == "search":
                return self._search_documents(**kwargs)
            elif action == "fill_fields":
                return self._fill_template_fields(**kwargs)
            elif action == "list":
                return self._list_documents()
            elif action == "clear":
                return self._clear_documents()
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}\næ”¯æŒçš„æ“ä½œ: upload, search, fill_fields, list, clear"
        
        except Exception as e:
            logger.error(f"RAGæ“ä½œå¤±è´¥: {e}")
            return f"âŒ æ“ä½œå¤±è´¥: {str(e)}"
    
    def _upload_document(self, file_path: str, filename: str = "") -> str:
        """ä¸Šä¼ æ–‡æ¡£è¿›è¡Œembeddingå¤„ç†"""
        try:
            if not filename:
                filename = os.path.basename(file_path)
            
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename}")
            
            # 1. æå–æ–‡æ¡£å†…å®¹
            content = self.extractor.extract_content(file_path)
            
            if not content.strip():
                return f"âŒ æ–‡æ¡£å†…å®¹ä¸ºç©º: {filename}"
            
            # 2. ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(f"{filename}_{datetime.now().isoformat()}".encode()).hexdigest()
            
            # 3. æ·»åŠ åˆ°å‘é‡åº“
            metadata = {
                "filename": filename,
                "file_path": file_path,
                "upload_time": datetime.now().isoformat(),
                "content_length": len(content)
            }
            
            chunks_count = self.vector_store.add_document(doc_id, content, metadata)
            
            result = f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼\n\n"
            result += f"ğŸ“„ æ–‡ä»¶å: {filename}\n"
            result += f"ğŸ†” æ–‡æ¡£ID: {doc_id}\n"
            result += f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\n"
            result += f"ğŸ”¢ åˆ†å—æ•°é‡: {chunks_count} ä¸ª\n"
            result += f"â° ä¸Šä¼ æ—¶é—´: {metadata['upload_time']}\n"
            result += f"ğŸ” å·²å®Œæˆembeddingå¤„ç†ï¼Œå¯ç”¨äºæ™ºèƒ½æœç´¢"
            
            logger.info(f"âœ… æ–‡æ¡£ {filename} å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
            return f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}"
    
    def _search_documents(self, query: str) -> str:
        """æœç´¢ç›¸å…³æ–‡æ¡£å†…å®¹"""
        try:
            logger.info(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
            
            results = self.vector_store.search_documents(query, n_results=5)
            
            if not results:
                return f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n\nâŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
            
            result = f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n\n"
            result += f"ğŸ“‹ æ‰¾åˆ° {len(results)} æ¡ç›¸å…³å†…å®¹ï¼š\n\n"
            
            for i, doc in enumerate(results, 1):
                result += f"ğŸ“„ ç»“æœ {i}:\n"
                result += f"   ğŸ“ å†…å®¹: {doc['content'][:200]}{'...' if len(doc['content']) > 200 else ''}\n"
                result += f"   ğŸ“ æ¥æº: {doc['metadata'].get('filename', 'æœªçŸ¥')}\n"
                result += f"   ğŸ¯ ç›¸ä¼¼åº¦: {1 - doc['distance']:.3f}\n\n"
            
            return result
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    def _fill_template_fields(self, template_fields_json: Dict[str, str]) -> str:
        """åŸºäºæ¨¡æ¿å­—æ®µJSONè¿›è¡Œæ™ºèƒ½å¡«å…… - æ ¸å¿ƒåŠŸèƒ½"""
        try:
            logger.info("ğŸ¯ å¼€å§‹æ¨¡æ¿å­—æ®µæ™ºèƒ½å¡«å……")
            
            # éªŒè¯è¾“å…¥
            if not isinstance(template_fields_json, dict):
                return "âŒ template_fields_json å¿…é¡»æ˜¯å­—å…¸æ ¼å¼"
            
            if not template_fields_json:
                return "âŒ template_fields_json ä¸èƒ½ä¸ºç©º"
            
            # æ‰§è¡Œæ™ºèƒ½å¡«å……
            filled_fields = self.field_processor.fill_template_fields(
                template_fields_json, self.vector_store
            )
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            result = f"âœ… æ¨¡æ¿å­—æ®µæ™ºèƒ½å¡«å……å®Œæˆï¼\n\n"
            result += f"ğŸ“‹ è¾“å…¥å­—æ®µ: {len(template_fields_json)} ä¸ª\n"
            result += f"ğŸ“ å¡«å……å­—æ®µ: {len(filled_fields)} ä¸ª\n\n"
            result += "ğŸ“„ å¡«å……ç»“æœ:\n"
            result += "=" * 50 + "\n"
            
            for field_name, filled_content in filled_fields.items():
                result += f"ğŸ”¸ {field_name}:\n"
                result += f"   {filled_content[:100]}{'...' if len(filled_content) > 100 else ''}\n\n"
            
            result += "=" * 50 + "\n"
            result += f"ğŸ’¾ å®Œæ•´å¡«å……ç»“æœJSON:\n{json.dumps(filled_fields, ensure_ascii=False, indent=2)}"
            
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡æ¿å­—æ®µå¡«å……å¤±è´¥: {e}")
            return f"âŒ æ¨¡æ¿å­—æ®µå¡«å……å¤±è´¥: {str(e)}"
    
    def _list_documents(self) -> str:
        """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£"""
        try:
            documents = self.vector_store.get_all_documents()
            
            if not documents:
                return "ğŸ“š å½“å‰æ²¡æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£"
            
            result = f"ğŸ“š å·²ä¸Šä¼ æ–‡æ¡£åˆ—è¡¨ (å…± {len(documents)} ä¸ª):\n\n"
            
            for i, doc in enumerate(documents, 1):
                result += f"ğŸ“„ æ–‡æ¡£ {i}:\n"
                result += f"   ğŸ“ æ–‡ä»¶å: {doc['metadata'].get('filename', 'æœªçŸ¥')}\n"
                result += f"   ğŸ†” æ–‡æ¡£ID: {doc['id']}\n"
                result += f"   ğŸ“Š åˆ†å—æ•°: {doc['chunks']} ä¸ª\n"
                result += f"   â° ä¸Šä¼ æ—¶é—´: {doc['metadata'].get('upload_time', 'æœªçŸ¥')}\n\n"
            
            return result
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"
    
    def _clear_documents(self) -> str:
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£"""
        try:
            self.vector_store.clear_all()
            return "âœ… æ‰€æœ‰æ–‡æ¡£å·²æ¸…ç©º"
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ–‡æ¡£å¤±è´¥: {e}")
            return f"âŒ æ¸…ç©ºæ–‡æ¡£å¤±è´¥: {str(e)}" 