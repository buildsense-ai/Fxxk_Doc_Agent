#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡æ¿æ’å…¥åŠŸèƒ½æ¨¡å—ï¼šAIæ™ºèƒ½åˆå¹¶åŸå§‹æ–‡æ¡£ä¸æ¨¡æ¿JSONï¼ˆFunction Callç‰ˆæœ¬ï¼‰
æ”¹é€ ä¸ºçº¯function callå½¢å¼ï¼Œä¾›ä¸»åè°ƒä»£ç†è°ƒç”¨
"""

import os
import json
import logging
import traceback
import tempfile
import subprocess
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple
from pathlib import Path

from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from openai import OpenAI
import fitz  # PyMuPDF
from docx import Document as DocxDocument

# å†…ç½®ä¸“ä¸š prompt å‡½æ•°ï¼ˆä» prompt_utils.py å¤åˆ¶ï¼‰
def get_fill_data_prompt(template_structure: str, placeholders: str, input_data: str) -> str:
    """
    Generates a prompt for the AI to handle hybrid mapping: both template structure and placeholders.
    """
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ··åˆæ¨¡å¼æ–‡æ¡£å¡«å†™åŠ©æ‰‹ã€‚ä½ éœ€è¦åŒæ—¶å¤„ç†ä¸‰ç§ä»»åŠ¡ï¼š
1. **æ¨¡æ¿ç»“æ„åŒ¹é…**ï¼šå°†JSONæ•°æ®æ˜ å°„åˆ°Wordæ–‡æ¡£çš„è¡¨æ ¼å•å…ƒæ ¼å’Œæ®µè½ç»“æ„ä¸­
2. **å ä½ç¬¦åŒ¹é…**ï¼šä¸ºç‰¹å®šçš„å ä½ç¬¦æ‰¾åˆ°å¯¹åº”çš„æ•°æ®å€¼
3. **å›¾ç‰‡å¼•ç”¨å¤„ç†**ï¼šå¤„ç†å›¾ç‰‡å ä½ç¬¦ï¼Œç¡®ä¿å›¾ç‰‡èƒ½æ­£ç¡®å¼•ç”¨

---

**æ ¸å¿ƒä»»åŠ¡ï¼šç”Ÿæˆç»Ÿä¸€çš„å¡«å……æ•°æ®æ˜ å°„**

ä½ å°†è·å¾—ï¼š
- **æ¨¡æ¿ç»“æ„**ï¼šæ–‡æ¡£ä¸­æ‰€æœ‰å•å…ƒæ ¼å’Œæ®µè½çš„ç»“æ„åŒ–è¡¨ç¤º
- **å ä½ç¬¦åˆ—è¡¨**ï¼šä»ç‰¹å®šæ¨¡å¼ï¼ˆå¦‚"é¡¹ç›®åç§°ï¼š"å’Œ"è‡´____ï¼ˆç›‘ç†å•ä½ï¼‰"ï¼‰æå–çš„å ä½ç¬¦
- **è¾“å…¥æ•°æ®**ï¼šéœ€è¦æ˜ å°„çš„JSONæ•°æ®ï¼ˆå¯èƒ½åŒ…å«attachments_mapå›¾ç‰‡ä¿¡æ¯ï¼‰

ä½ çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä¸‰ç§ç±»å‹çš„æ˜ å°„ï¼š
- **ç»“æ„æ˜ å°„**ï¼šé”®å¦‚"table_0_row_1_col_1"æˆ–"paragraph_3"ï¼Œç”¨äºå¡«å……æ¨¡æ¿ç»“æ„
- **å ä½ç¬¦æ˜ å°„**ï¼šé”®å¦‚"label_é¡¹ç›®åç§°"æˆ–"inline_ç›‘ç†å•ä½"ï¼Œç”¨äºæ›¿æ¢å ä½ç¬¦
- **å›¾ç‰‡æ˜ å°„**ï¼šå¦‚æœè¾“å…¥æ•°æ®åŒ…å«attachments_mapï¼Œç›´æ¥å°†å…¶åŒ…å«åœ¨è¾“å‡ºä¸­

---

**é‡è¦åŸåˆ™ï¼šåªå¡«å……ç¡®å®šçš„æ•°æ®**

âš ï¸ **å…³é”®è¦æ±‚**ï¼š
- **åªæœ‰åœ¨è¾“å…¥æ•°æ®ä¸­æ‰¾åˆ°æ˜ç¡®å¯¹åº”çš„ä¿¡æ¯æ—¶ï¼Œæ‰è¾“å‡ºè¯¥é”®å€¼å¯¹**
- **å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„æ•°æ®ï¼Œè¯·å®Œå…¨çœç•¥è¯¥é”®ï¼Œä¸è¦è¾“å‡ºä»»ä½•å ä½æ–‡æœ¬å¦‚"å¾…å¡«å†™"ã€"____"ç­‰**
- **å®å¯ç•™ç©ºï¼Œä¹Ÿä¸è¦å¡«å…¥ä¸ç¡®å®šçš„å†…å®¹**

---

**æ˜ å°„è§„åˆ™**

### 1. æ¨¡æ¿ç»“æ„æ˜ å°„ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
- **æ’é™¤è§„åˆ™**ï¼šå¦‚æœå•å…ƒæ ¼æˆ–æ®µè½æ–‡æœ¬ä¸­åŒ…å« **"ï¼ˆç­¾å­—ï¼‰"** å­—æ ·ï¼Œ**ç»å¯¹ä¸è¦**ä¸ºå…¶å¡«å……ä»»ä½•å†…å®¹ã€‚
- å¦‚æœä¸€ä¸ªå•å…ƒæ ¼çš„å†…å®¹æ˜¯"æ ‡ç­¾ï¼š"ï¼ˆä¾‹å¦‚ `"é¡¹ç›®åç§°ï¼š"`ï¼‰ï¼Œå¹¶ä¸”å…¶å³ä¾§æˆ–ä¸‹æ–¹çš„å•å…ƒæ ¼ä¸ºç©ºï¼Œåˆ™åº”å°†æ•°æ®å¡«å…¥é‚£ä¸ªç©ºå•å…ƒæ ¼
- å¦‚æœä¸€ä¸ªå•å…ƒæ ¼åŒ…å«ä¸‹åˆ’çº¿å ä½ç¬¦ä½†**ä¸æ˜¯ç‰¹å®šçš„ä¸¤ç§æ¨¡å¼**ï¼Œåˆ™ç›´æ¥æ›¿æ¢è¯¥å•å…ƒæ ¼å†…å®¹
- æ™ºèƒ½åŒ¹é…ï¼šè¿ç”¨æ¨ç†èƒ½åŠ›ï¼Œå¦‚`project_leader`åº”æ˜ å°„åˆ°"é¡¹ç›®è´Ÿè´£äºº"ç›¸å…³çš„ä½ç½®

### 2. å ä½ç¬¦æ˜ å°„ï¼ˆç°æœ‰é€»è¾‘ï¼‰
- å¯¹äº`label_*`ç±»å‹çš„å ä½ç¬¦ï¼ˆæ¥è‡ª"é¡¹ç›®åç§°ï¼š"ï¼‰ï¼Œæ‰¾åˆ°è¯­ä¹‰åŒ¹é…çš„æ•°æ®
- å¯¹äº`inline_*`ç±»å‹çš„å ä½ç¬¦ï¼ˆæ¥è‡ª"è‡´____ï¼ˆç›‘ç†å•ä½ï¼‰"ï¼‰ï¼Œæ‰¾åˆ°å¯¹åº”çš„å€¼
- å ä½ç¬¦åç§°æ˜¯æç¤ºæ€§çš„ï¼Œéœ€è¦æ™ºèƒ½åŒ¹é…åˆ°è¾“å…¥æ•°æ®çš„å­—æ®µ
- **å¦‚æœç¡®å®æ‰¾ä¸åˆ°åŒ¹é…çš„æ•°æ®ï¼Œå°±ä¸è¦è¾“å‡ºè¿™ä¸ªé”®**

### 3. å›¾ç‰‡æ˜ å°„ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
- å¦‚æœè¾“å…¥æ•°æ®åŒ…å«`attachments_map`å­—æ®µï¼Œ**å¿…é¡»**å°†å…¶å®Œæ•´åœ°åŒ…å«åœ¨è¾“å‡ºJSONä¸­
- è¿™å°†ç¡®ä¿å›¾ç‰‡ä¿¡æ¯èƒ½è¢«æ­£ç¡®ä¼ é€’åˆ°æ–‡æ¡£ç”Ÿæˆé˜¶æ®µ
- å›¾ç‰‡å ä½ç¬¦`{{{{image:key}}}}`ä¼šåœ¨æ–‡æ¡£ç”Ÿæˆæ—¶è¢«è‡ªåŠ¨æ›¿æ¢ä¸º"ï¼ˆè¯¦è§é™„ä»¶Nï¼‰"çš„æ ¼å¼

---

### ğŸ§ª ç¤ºä¾‹

**æ¨¡æ¿ç»“æ„:**
```json
{{{{
  "table_0_row_0_col_0": "é¡¹ç›®åç§°ï¼š",
  "table_0_row_0_col_1": "",
  "table_0_row_1_col_0": "è´Ÿè´£äººï¼š",
  "table_0_row_1_col_1": "",
  "paragraph_0": "é¡¹ç›®åç§°ï¼š{{{{label_é¡¹ç›®åç§°}}}}",
  "paragraph_1": "è‡´{{{{inline_ç›‘ç†å•ä½}}}}ï¼ˆç›‘ç†å•ä½ï¼‰",
  "paragraph_2": "æ–½å·¥å›¾è¯¦è§ï¼š{{{{image:shiGongTu}}}}"
}}}}
```

**å ä½ç¬¦åˆ—è¡¨:**
```json
[
  "label_é¡¹ç›®åç§°",
  "inline_ç›‘ç†å•ä½"
]
```

**è¾“å…¥æ•°æ®:**
```json
{{{{
  "project_name": "å¤å»ºç­‘ä¿æŠ¤ä¿®ç¼®å·¥ç¨‹",  
  "project_leader": "å¼ ä¸‰",
  "supervision_company": "å¹¿å·å»ºè®¾ç›‘ç†å…¬å¸",
  "attachments_map": {{{{
    "shiGongTu": "uploads/construction_drawing.png",
    "xianChangZhaoPian": "uploads/site_photo.jpg"
  }}}}
}}}}
```

**è¾“å‡ºç»“æœ:**
```json
{{{{
  "table_0_row_0_col_1": "å¤å»ºç­‘ä¿æŠ¤ä¿®ç¼®å·¥ç¨‹",
  "table_0_row_1_col_1": "å¼ ä¸‰",
  "label_é¡¹ç›®åç§°": "å¤å»ºç­‘ä¿æŠ¤ä¿®ç¼®å·¥ç¨‹",
  "inline_ç›‘ç†å•ä½": "å¹¿å·å»ºè®¾ç›‘ç†å…¬å¸",
  "attachments_map": {{{{
    "shiGongTu": "uploads/construction_drawing.png",
    "xianChangZhaoPian": "uploads/site_photo.jpg"
  }}}}
}}}}
```

---

ç°åœ¨è¯·æ ¹æ®ä¸‹æ–¹çš„æ•°æ®è¿›è¡Œæ··åˆæ˜ å°„ï¼š

**æ¨¡æ¿ç»“æ„:**
```json
{template_structure}
```

**å ä½ç¬¦åˆ—è¡¨:**
```json
{placeholders}
```

**è¾“å…¥æ•°æ®:**
```json
{input_data}
```

**é‡è¦è¦æ±‚:**
- **æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡**
- è¾“å‡ºç»Ÿä¸€çš„JSONå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰ç±»å‹çš„æ˜ å°„
- å¯¹å ä½ç¬¦è¿›è¡Œæ™ºèƒ½è¯­ä¹‰åŒ¹é…
- ä¿æŒåŸæœ‰çš„æ¨¡æ¿ç»“æ„å¡«å……é€»è¾‘
- **å¦‚æœè¾“å…¥æ•°æ®åŒ…å«attachments_mapï¼Œå¿…é¡»å®Œæ•´åŒ…å«åœ¨è¾“å‡ºä¸­**
- **é‡è¦ï¼šå¦‚æœæ‰¾ä¸åˆ°å¯¹åº”æ•°æ®ï¼Œå®Œå…¨çœç•¥è¯¥é”®ï¼Œä¸è¦å¡«å…¥"å¾…å¡«å†™"ç­‰å ä½æ–‡æœ¬**
- åªè¾“å‡ºæœ€ç»ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«è§£é‡Šè¯´æ˜æˆ–Markdownæ ¼å¼
"""

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv()
    logger.info("âœ… å·²åŠ è½½.envç¯å¢ƒå˜é‡æ–‡ä»¶")
except ImportError:
    logger.warning("âš ï¸ python-dotenvæœªå®‰è£…ï¼Œå°†ç›´æ¥ä»ç³»ç»Ÿç¯å¢ƒå˜é‡è¯»å–é…ç½®")
except Exception as e:
    logger.warning(f"âš ï¸ åŠ è½½.envæ–‡ä»¶æ—¶å‡ºç°é—®é¢˜: {e}")

def get_api_key() -> str:
    """è·å–OpenRouter APIå¯†é’¥"""
    api_key = os.environ.get("OPENROUTER_API_KEY")
    if not api_key:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
        test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
        if test_mode:
            logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAPIå¯†é’¥")
            return "test-api-key-for-testing"
        
        logger.error("âŒ æœªæ‰¾åˆ°OPENROUTER_API_KEY")
        raise RuntimeError("ç¼ºå°‘å¿…éœ€çš„APIå¯†é’¥é…ç½®")
    return api_key

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUTPUT_DIR = "generated_docs"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

class ProcessingError(Exception):
    """è‡ªå®šä¹‰å¤„ç†å¼‚å¸¸"""
    def __init__(self, message: str, error_code: str, status_code: int = 500):
        self.message = message
        self.error_code = error_code
        self.status_code = status_code
        super().__init__(self.message)

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
    
    def __init__(self):
        logger.info("ğŸ“„ æ–‡æ¡£æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_from_file_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–å†…å®¹ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        if not os.path.exists(file_path):
            raise ProcessingError(
                f"åŸå§‹æ–‡æ¡£ä¸å­˜åœ¨: {file_path}",
                "FILE_NOT_FOUND",
                404
            )
        return self._extract_content(file_path)
    
    def extract_from_content(self, content: str) -> str:
        """ä»å·²æœ‰å†…å®¹ä¸­æå–å’Œæ¸…ç†ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        if not content or not content.strip():
            raise ProcessingError(
                "æä¾›çš„å†…å®¹ä¸ºç©º",
                "EMPTY_CONTENT",
                422
            )
        return content.strip()
    
    def convert_doc_to_docx(self, doc_path: str) -> str:
        """ä½¿ç”¨LibreOfficeå°†.docæ–‡ä»¶è½¬æ¢ä¸º.docxæ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ”„ å¼€å§‹DOCåˆ°DOCXè½¬æ¢...")
        
        if not os.path.exists(doc_path):
            logger.error(f"âŒ DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
            raise ProcessingError(f"DOCæ–‡ä»¶ä¸å­˜åœ¨: {doc_path}", "FILE_NOT_FOUND", 404)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        docx_path = doc_path.replace('.doc', '_converted.docx')
        
        try:
            # æ£€æŸ¥LibreOfficeæ˜¯å¦å¯ç”¨
            logger.info("ğŸ” æ£€æŸ¥LibreOfficeå¯ç”¨æ€§...")
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„LibreOfficeè·¯å¾„
            libreoffice_paths = [
                r'C:\Program Files\LibreOffice\program\soffice.exe',  # Windows 64ä½
                r'C:\Program Files (x86)\LibreOffice\program\soffice.exe',  # Windows 32ä½
                '/Applications/LibreOffice.app/Contents/MacOS/soffice',  # macOS
                'libreoffice',  # Linux/Windows PATH
                'soffice',  # å¤‡ç”¨å‘½ä»¤
            ]
            
            libreoffice_cmd = None
            for path in libreoffice_paths:
                # å¯¹äºç»å¯¹è·¯å¾„ï¼Œå…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.isabs(path):  # ä½¿ç”¨os.path.isabsæ›´å‡†ç¡®
                    if os.path.exists(path):
                        # æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨
                        libreoffice_cmd = path
                        logger.info(f"âœ… æ‰¾åˆ°LibreOffice: {path}")
                        break
                    else:
                        logger.debug(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
                        continue
                else:
                    # å¯¹äºPATHä¸­çš„å‘½ä»¤ï¼Œå°è¯•ç®€å•æ£€æŸ¥
                    try:
                        result = subprocess.run([path, '--help'], 
                                              capture_output=True, 
                                              text=True, 
                                              timeout=3)
                        if result.returncode == 0:
                            libreoffice_cmd = path
                            logger.info(f"âœ… æ‰¾åˆ°LibreOffice: {path}")
                            break
                    except (FileNotFoundError, subprocess.TimeoutExpired, OSError):
                        logger.debug(f"æ— æ³•ä½¿ç”¨è·¯å¾„: {path}")
                        continue
            
            if not libreoffice_cmd:
                logger.error("âŒ æœªæ‰¾åˆ°LibreOfficeï¼Œè¯·ç¡®ä¿å·²å®‰è£…LibreOffice")
                raise ProcessingError("LibreOfficeæœªå®‰è£…æˆ–ä¸å¯ç”¨", "LIBREOFFICE_NOT_FOUND", 500)
            
            # æ‰§è¡Œè½¬æ¢
            logger.info(f"ğŸ“„ æ­£åœ¨è½¬æ¢: {doc_path} -> {docx_path}")
            
            # åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶
            if os.path.exists(docx_path):
                os.remove(docx_path)
                logger.info("ğŸ—‘ï¸ åˆ é™¤å·²å­˜åœ¨çš„è½¬æ¢æ–‡ä»¶")
            
            # LibreOfficeè½¬æ¢å‘½ä»¤
            cmd = [
                libreoffice_cmd,
                '--headless',
                '--convert-to', 'docx',
                '--outdir', os.path.dirname(doc_path),
                doc_path
            ]
            
            logger.info(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=30)
            
            if result.returncode != 0:
                logger.error(f"âŒ LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                raise ProcessingError(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}", "LIBREOFFICE_CONVERSION_FAILED", 500)
            
            # æ£€æŸ¥è½¬æ¢åçš„æ–‡ä»¶
            expected_docx = doc_path.replace('.doc', '.docx')
            if os.path.exists(expected_docx):
                # é‡å‘½åä¸ºæˆ‘ä»¬æœŸæœ›çš„æ–‡ä»¶å
                if expected_docx != docx_path:
                    os.rename(expected_docx, docx_path)
                
                logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {docx_path}")
                return docx_path
            else:
                logger.error(f"âŒ è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°: {expected_docx}")
                raise ProcessingError("è½¬æ¢åçš„æ–‡ä»¶æœªæ‰¾åˆ°", "CONVERSION_OUTPUT_NOT_FOUND", 500)
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ LibreOfficeè½¬æ¢è¶…æ—¶")
            raise ProcessingError("LibreOfficeè½¬æ¢è¶…æ—¶", "LIBREOFFICE_TIMEOUT", 500)
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise ProcessingError(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", "CONVERSION_ERROR", 500)
    
    def _extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹çš„æ ¸å¿ƒæ–¹æ³•ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        logger.info(f"ğŸ“„ å¼€å§‹æå–æ–‡æ¡£å†…å®¹: {Path(file_path).name}")
        
        content = ""
        converted_file = None  # Track converted file for cleanup
        
        try:
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.doc':
                # Convert .doc to .docx using LibreOffice
                logger.info("ğŸ”„ æ£€æµ‹åˆ°.docæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢ä¸º.docx...")
                converted_file = self.convert_doc_to_docx(file_path)
                file_path = converted_file  # Use converted file for extraction
                file_ext = '.docx'  # Update extension
            
            if file_ext == '.docx':
                doc = DocxDocument(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
                
                # æå–è¡¨æ ¼å†…å®¹
                for table in doc.tables:
                    for row in table.rows:
                        row_text = " | ".join([cell.text.strip() for cell in row.cells])
                        if row_text.strip():
                            content += f"\nè¡¨æ ¼è¡Œ: {row_text}"
            
            elif file_ext == '.pdf':
                doc = fitz.open(file_path)
                for page in doc:
                    content += page.get_text()
                doc.close()
            
            elif file_ext in ['.txt', '.md']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            else:
                raise ProcessingError(
                    f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}",
                    "UNSUPPORTED_FORMAT",
                    422
                )
            
            if not content.strip():
                raise ProcessingError(
                    "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                    "EMPTY_DOCUMENT",
                    422
                )
            
            logger.info(f"âœ… æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # Cleanup converted file if it exists
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶: {converted_file}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶å¤±è´¥: {e}")
            
            return content.strip()
            
        except ProcessingError:
            # Cleanup converted file on error
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶: {converted_file}")
                except:
                    pass
            raise
        except Exception as e:
            # Cleanup converted file on error
            if converted_file and os.path.exists(converted_file):
                try:
                    os.remove(converted_file)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†è½¬æ¢æ–‡ä»¶: {converted_file}")
                except:
                    pass
            logger.error(f"âŒ æå–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            raise ProcessingError(
                f"æ–‡æ¡£å†…å®¹æå–å¤±è´¥: {str(e)}",
                "EXTRACTION_ERROR",
                500
            )

class ContentMerger:
    """å†…å®¹æ™ºèƒ½åˆå¹¶å™¨ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½å¹¶å¢å¼ºï¼‰"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.model = "google/gemini-2.5-pro-preview"  # ä¿æŒä½¿ç”¨åŸæ¥çš„æ¨¡å‹
        logger.info("ğŸ§  å†…å®¹åˆå¹¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """ä½¿ç”¨AIæ™ºèƒ½åˆå¹¶æ¨¡æ¿JSONå’ŒåŸå§‹å†…å®¹ï¼ˆä½¿ç”¨ä¸“ä¸špromptï¼‰"""
        logger.info("ğŸ§  å¼€å§‹AIæ™ºèƒ½åˆå¹¶ï¼ˆä½¿ç”¨ä¸“ä¸špromptï¼‰...")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
        test_mode = os.environ.get("TEST_MODE", "false").lower() == "true"
        if test_mode or self.client.api_key == "test-api-key-for-testing":
            logger.warning("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹ŸAIåˆå¹¶")
            return self._mock_merge_content(template_json, original_content)
        
        # ä½¿ç”¨ prompt_utils ä¸­çš„ä¸“ä¸š prompt
        # ä¸ºäº†é€‚é…å‡½æ•°å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦å°† template_json è½¬æ¢ä¸ºæ¨¡æ¿ç»“æ„æ ¼å¼
        template_structure = json.dumps(template_json, ensure_ascii=False, indent=2)
        placeholders = json.dumps(list(template_json.keys()), ensure_ascii=False, indent=2)
        input_data = original_content
        
        prompt = get_fill_data_prompt(template_structure, placeholders, input_data)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            
            if not response or not response.choices or not response.choices[0].message.content:
                raise ProcessingError(
                    "AIå“åº”æ— æ•ˆæˆ–ä¸ºç©º",
                    "AI_NO_RESPONSE",
                    500
                )
            
            # æå–JSONå†…å®¹
            response_content = response.choices[0].message.content.strip()
            json_str = self._extract_json_from_response(response_content)
            
            try:
                merged_content = json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                logger.error(f"AIå“åº”å†…å®¹: {response_content}")
                raise ProcessingError(
                    f"AIè¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(e)}",
                    "AI_INVALID_JSON",
                    422
                )
            
            # éªŒè¯åˆå¹¶ç»“æœ
            if not isinstance(merged_content, dict):
                raise ProcessingError(
                    "AIè¿”å›çš„å†…å®¹ä¸æ˜¯å­—å…¸æ ¼å¼",
                    "AI_INVALID_FORMAT",
                    422
                )
            
            logger.info(f"âœ… AIåˆå¹¶æˆåŠŸï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
            for key, value in merged_content.items():
                preview = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                logger.info(f"   ğŸ“ {key}: {preview}")
            
            return merged_content
            
        except ProcessingError:
            raise
        except Exception as e:
            logger.error(f"âŒ AIåˆå¹¶å¤±è´¥: {e}")
            raise ProcessingError(
                f"AIåˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "AI_MERGE_ERROR",
                500
            )
    
    def analyze_missing_fields(self, merged_content: Dict[str, str]) -> List[str]:
        """åˆ†æåˆå¹¶å†…å®¹ä¸­çš„ç¼ºå¤±å­—æ®µï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ“Š åˆ†æç¼ºå¤±å­—æ®µ...")
        
        missing_fields = []
        
        for key, value in merged_content.items():
            if not value or len(str(value).strip()) < 10:
                missing_fields.append(key)
                logger.info(f"   âŒ ç¼ºå¤±å­—æ®µ: {key}")
            elif "å¾…å¡«å†™" in str(value) or "éœ€è¦è¡¥å……" in str(value) or "____" in str(value):
                missing_fields.append(key)
                logger.info(f"   âš ï¸ ä¸å®Œæ•´å­—æ®µ: {key}")
        
        logger.info(f"ğŸ“Š åˆ†æå®Œæˆï¼Œå‘ç° {len(missing_fields)} ä¸ªç¼ºå¤±æˆ–ä¸å®Œæ•´çš„å­—æ®µ")
        return missing_fields
    
    def _mock_merge_content(self, template_json: Dict[str, str], original_content: str) -> Dict[str, str]:
        """æ¨¡æ‹ŸAIåˆå¹¶ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ§ª æ¨¡æ‹ŸAIåˆå¹¶æ¨¡å¼")
        
        merged_content = {}
        content_lines = original_content.split('\n')
        content_preview = ' '.join(content_lines[:5])[:200]
        
        for key, description in template_json.items():
            # åŸºäºåŸå§‹å†…å®¹å’Œæ¨¡æ¿æè¿°ç”Ÿæˆç®€å•çš„åˆå¹¶å†…å®¹
            merged_content[key] = f"""æ ¹æ®åŸå§‹æ–‡æ¡£å†…å®¹ç”Ÿæˆçš„{key}ç« èŠ‚ï¼š

{description}

åŸºäºåŸå§‹æ–‡æ¡£çš„ç›¸å…³ä¿¡æ¯ï¼š
{content_preview}

æœ¬ç« èŠ‚å†…å®¹å·²æ ¹æ®æ¨¡æ¿è¦æ±‚è¿›è¡Œæ™ºèƒ½æ•´åˆï¼Œç¡®ä¿ç¬¦åˆå·¥ç¨‹æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼å’Œè¦æ±‚ã€‚å…·ä½“å†…å®¹åŒ…æ‹¬é¡¹ç›®çš„åŸºæœ¬æƒ…å†µã€æŠ€æœ¯è¦æ±‚ã€å®æ–½æ–¹æ¡ˆç­‰å…³é”®ä¿¡æ¯ã€‚

æ³¨ï¼šæ­¤å†…å®¹ç”±æµ‹è¯•æ¨¡å¼ç”Ÿæˆï¼Œå®é™…åº”ç”¨ä¸­å°†ä½¿ç”¨çœŸå®AIè¿›è¡Œæ™ºèƒ½åˆå¹¶ã€‚"""
        
        logger.info(f"âœ… æ¨¡æ‹Ÿåˆå¹¶å®Œæˆï¼Œç”Ÿæˆ {len(merged_content)} ä¸ªç« èŠ‚")
        return merged_content
    
    def _extract_json_from_response(self, response_content: str) -> str:
        """ä»AIå“åº”ä¸­æå–JSONå†…å®¹ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        # å°è¯•æå–JSON
        if "```json" in response_content:
            start = response_content.find("```json") + 7
            end = response_content.find("```", start)
            if end != -1:
                return response_content[start:end].strip()
            else:
                return response_content[response_content.find("```json") + 7:].strip()
        elif response_content.startswith("{") and response_content.endswith("}"):
            return response_content
        else:
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start_idx = response_content.find("{")
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(response_content[start_idx:], start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            return response_content[start_idx:i+1]
            return response_content

class TemplateInserter:
    """çœŸæ­£çš„æ¨¡æ¿æ’å…¥å™¨ - ä¿æŒåŸå§‹æ¨¡æ¿ç»“æ„"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        self.model = "google/gemini-2.5-pro-preview"
        self.placeholder_originals = {}  # Store original text of placeholders
        logger.info("ğŸ“„ æ¨¡æ¿æ’å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _preprocess_template_and_extract_placeholders(self, doc_path: str, output_path: str) -> List[str]:
        """
        æ‰©å±•å ä½ç¬¦é¢„å¤„ç†ï¼Œä»¥åŒ…å«é€šç”¨çš„ä¸‹åˆ’çº¿å­—æ®µï¼Œå¹¶ä¼˜åŒ–æ›¿æ¢é€»è¾‘
        """
        logger.info("ğŸ› ï¸  é˜¶æ®µ 0: å¼€å§‹æ‰©å±•å ä½ç¬¦é¢„å¤„ç†...")
        
        self.placeholder_originals = {} # Reset for each new template analysis
        doc = Document(doc_path)
        placeholders = set()
        blank_counter = 0 # Counter for generic underscore placeholders
        
        def process_text_and_extract_keys(text: str) -> (str, List[str]):
            nonlocal blank_counter
            found_keys = []

            def repl_func(match):
                nonlocal blank_counter
                # Pattern for 'è‡´...': underscore_str in group(1), hint in group(2)
                if match.group(1) is not None:
                    if "ï¼ˆç­¾å­—ï¼‰" in match.group(0) or "(ç­¾å­—)" in match.group(0):
                        return match.group(0)
                    
                    underscore_str = match.group(1)
                    hint = match.group(2)
                    placeholder_key = f"inline_{hint}"
                    found_keys.append(placeholder_key)
                    self.placeholder_originals[placeholder_key] = underscore_str
                    replacement = f"è‡´{{{placeholder_key}}}ï¼ˆ{hint}ï¼‰"
                    logger.info(f"   - å‘ç°å†…è”æ¨¡å¼: '{match.group(0)}' -> '{replacement}'")
                    return replacement

                # Pattern for 'label:': label in group(3)
                elif match.group(3) is not None:
                    # The regex now prevents matching 'ï¼ˆç­¾å­—ï¼‰:'
                    label = match.group(3).strip()
                    placeholder_key = f"label_{label}"
                    found_keys.append(placeholder_key)
                    replacement = f"{label}ï¼š{{{placeholder_key}}}"
                    logger.info(f"   - å‘ç°æ ‡ç­¾æ¨¡å¼: '{match.group(0)}' -> '{replacement}'")
                    return replacement

                # Pattern for general underscores: underscore_str in group(4)
                elif match.group(4) is not None:
                    underscore_str = match.group(4)
                    placeholder_key = f"blank_{blank_counter}"
                    found_keys.append(placeholder_key)
                    self.placeholder_originals[placeholder_key] = underscore_str
                    replacement = f"{{{placeholder_key}}}"
                    logger.info(f"   - å‘ç°é€šç”¨ä¸‹åˆ’çº¿æ¨¡å¼: '{underscore_str}' -> '{replacement}'")
                    blank_counter += 1
                    return replacement
                
                return match.group(0)

            # Regex updated to handle spaced underscores and avoid capturing signature labels
            import re
            pattern = re.compile(
                r"è‡´\s*(__{3,})\s*ï¼ˆ([^ï¼‰]+)ï¼‰"              # G1: underscore, G2: hint
                r"|([^ï¼š\nï¼ˆ(]+?)ï¼š\s*$"                    # G3: label, avoids '(...):'
                r"|((?:_{4,}[\s\xa0]*)+)"               # G4: general underscore blocks
            )

            processed_text = pattern.sub(repl_func, text)
            
            return processed_text, found_keys
        
        # --- Process all paragraphs ---
        for para in doc.paragraphs:
            original_text = para.text
            if not original_text.strip():
                continue

            new_text, keys = process_text_and_extract_keys(original_text)
            if new_text != original_text:
                placeholders.update(keys)
                # To preserve formatting, we clear runs and add a new one
                para.clear()
                para.add_run(new_text)
                logger.info(f"   ğŸ“ æ®µè½æ›´æ–°: '{original_text.strip()}' -> '{new_text.strip()}'")

        # --- Process all tables ---
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    original_text = cell.text
                    if not original_text.strip():
                        continue
                        
                    new_text, keys = process_text_and_extract_keys(original_text)
                    if new_text != original_text:
                        placeholders.update(keys)
                        # Reverted to cell.text for simplicity and correctness.
                        # This replaces the content of the first paragraph in the cell.
                        cell.text = new_text
                        logger.info(f"   ğŸ“‹ è¡¨æ ¼æ›´æ–°: '{original_text.strip()}' -> '{new_text.strip()}'")
        
        doc.save(output_path)
        logger.info(f"âœ… æ‰©å±•é¢„å¤„ç†å®Œæˆ. æ‰¾åˆ° {len(placeholders)} ä¸ªå ä½ç¬¦. æ–°æ¨¡æ¿: {output_path}")
        return list(placeholders)

    def analyze_template_structure(self, template_path: str) -> Dict[str, str]:
        """
        ç¡®å®šæ€§åœ°åˆ†æWordæ¨¡æ¿ï¼Œæå–å¸¦æœ‰ä½ç½®ä¿¡æ¯çš„ç»“æ„ã€‚
        """
        logger.info("ğŸ” å¼€å§‹ç¡®å®šæ€§æ¨¡æ¿ç»“æ„åˆ†æ...")
        
        try:
            doc = Document(template_path)
            template_structure = {}
            
            logger.info(f"ğŸ“„ æ­£åœ¨è¯»å–æ¨¡æ¿æ–‡ä»¶: {template_path}")
            
            # æå–è¡¨æ ¼ç»“æ„
            for i, table in enumerate(doc.tables):
                for j, row in enumerate(table.rows):
                    for k, cell in enumerate(row.cells):
                        cell_key = f"table_{i}_row_{j}_col_{k}"
                        template_structure[cell_key] = cell.text.strip()
            
            # æå–æ®µè½ç»“æ„ï¼ˆä¸åšç‰¹æ®Šå¤„ç†ï¼Œä¿æŒåŸå§‹å†…å®¹ï¼‰
            for i, para in enumerate(doc.paragraphs):
                para_key = f"paragraph_{i}"
                template_structure[para_key] = para.text.strip()
            
            logger.info(f"âœ… æˆåŠŸæå– {len(template_structure)} ä¸ªç»“æ„å…ƒç´ ã€‚")
            
            return template_structure
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿ç»“æ„åˆ†æé”™è¯¯: {e}")
            raise

    def fill_template_with_ai_data(self, template_path: str, output_path: str, fill_data: Dict[str, str]):
        """
        æ··åˆå¡«å…… - æ”¯æŒå›¾ç‰‡é™„ä»¶å’Œå ä½ç¬¦ï¼Œä¿æŒåŸå§‹æ¨¡æ¿ç»“æ„
        """
        logger.info("ğŸ“ å¼€å§‹æ··åˆæ¨¡å¼æ¨¡æ¿å¡«å……ï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰...")
        
        doc = Document(template_path)
        filled_count = 0
        
        # 1. å‡†å¤‡é™„ä»¶ä¿¡æ¯
        attachments_map = fill_data.pop('attachments_map', {})
        attachment_ref_map = {}
        ordered_attachments = []
        if attachments_map and isinstance(attachments_map, dict):
            logger.info(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(attachments_map)} ä¸ªå›¾ç‰‡é™„ä»¶å¾…å¤„ç†ã€‚")
            ordered_attachments = list(attachments_map.items())
            for i, (key, _) in enumerate(ordered_attachments):
                attachment_ref_map[key.strip()] = i + 1
        else:
            attachments_map = {}

        # 2. åˆ†ç¦»æ–‡æœ¬å¡«å……æ•°æ®
        placeholder_data = {k: v for k, v in fill_data.items() if k.startswith(('label_', 'inline_', 'blank_'))}
        structure_data = {k: v for k, v in fill_data.items() if k.startswith(('table_', 'paragraph_'))}
        
        # 3. æ›¿æ¢æ‰€æœ‰æ–‡æœ¬å ä½ç¬¦ï¼ˆåŒ…æ‹¬å›¾ç‰‡å¼•ç”¨ï¼‰
        import re
        image_placeholder_pattern = re.compile(r'\{\{image:([^}]+)\}\}')
        text_placeholder_pattern = re.compile(r'\{(label_[^}]+|inline_[^}]+|blank_[^}]+)\}')

        def process_element_text(element):
            nonlocal filled_count
            if '{' not in element.text:
                return

            original_text = element.text
            new_text = ""
            last_end = 0

            # åˆ›å»ºä¸€ä¸ªç»„åˆçš„æ­£åˆ™è¡¨è¾¾å¼æ¥æŸ¥æ‰¾æ‰€æœ‰ç±»å‹çš„å ä½ç¬¦
            combined_pattern = re.compile(f"({image_placeholder_pattern.pattern}|{text_placeholder_pattern.pattern})")
            
            for match in combined_pattern.finditer(original_text):
                new_text += original_text[last_end:match.start()]
                last_end = match.end()
                
                image_key_match = image_placeholder_pattern.match(match.group(0))
                text_key_match = text_placeholder_pattern.match(match.group(0))

                if image_key_match:
                    key = image_key_match.group(1).strip()
                    if key in attachment_ref_map:
                        number = attachment_ref_map[key]
                        replacement = f"ï¼ˆè¯¦è§é™„ä»¶{number}ï¼‰"
                        new_text += replacement
                        logger.info(f"   ğŸ–¼ï¸  å›¾ç‰‡å¼•ç”¨æ›¿æ¢: '{match.group(0)}' -> '{replacement}'")
                    else:
                        logger.warning(f"   âš ï¸  æ‰¾åˆ°å›¾ç‰‡å ä½ç¬¦ {match.group(0)} ä½†æ— åŒ¹é…å›¾ç‰‡ï¼Œå·²ç§»é™¤ã€‚")
                
                elif text_key_match:
                    placeholder_key = text_key_match.group(1)
                    placeholder = f"{{{placeholder_key}}}"

                    if placeholder_key in placeholder_data:
                        value = str(placeholder_data[placeholder_key])
                        new_text += value
                        logger.info(f"   âœï¸  å ä½ç¬¦å¡«å……: {placeholder} -> {value[:50]}...")
                        filled_count += 1
                    else: # æœªåŒ¹é…çš„æ–‡æœ¬å ä½ç¬¦
                        if placeholder_key.startswith('label_'):
                            logger.info(f"   ğŸ”˜  ç§»é™¤æœªåŒ¹é…æ ‡ç­¾å ä½ç¬¦: {placeholder}")
                        elif placeholder_key.startswith(('inline_', 'blank_')):
                            original_underscore = self.placeholder_originals.get(placeholder_key, '____')
                            new_text += original_underscore
                            logger.info(f"   ğŸ”˜  æ¢å¤æœªåŒ¹é…å ä½ç¬¦: {placeholder} -> '{original_underscore}'")

            new_text += original_text[last_end:]
            
            if new_text != original_text:
                element.text = new_text

        # éå†æ®µè½å’Œè¡¨æ ¼è¿›è¡Œç»Ÿä¸€æ›¿æ¢
        for para in doc.paragraphs:
            process_element_text(para)
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    process_element_text(cell)

        # 4. å¡«å……åŸå§‹ç»“æ„
        for i, table in enumerate(doc.tables):
            for j, row in enumerate(table.rows):
                for k, cell in enumerate(row.cells):
                    cell_key = f"table_{i}_row_{j}_col_{k}"
                    if cell_key in structure_data:
                        cell.text = str(structure_data[cell_key])
                        logger.info(f"   âœï¸  ç»“æ„å¡«å……(è¡¨æ ¼): {cell_key} -> {str(structure_data[cell_key])[:50]}...")
                        filled_count += 1

        for i, para in enumerate(doc.paragraphs):
            para_key = f"paragraph_{i}"
            if para_key in structure_data:
                # åªæœ‰åœ¨æ®µè½ä¸­æ²¡æœ‰å ä½ç¬¦çš„æƒ…å†µä¸‹æ‰è¿›è¡Œç»“æ„å¡«å……
                combined_pattern = re.compile(f"({image_placeholder_pattern.pattern}|{text_placeholder_pattern.pattern})")
                if not combined_pattern.search(para.text):
                    para.text = str(structure_data[para_key])
                    logger.info(f"   âœï¸  ç»“æ„å¡«å……(æ®µè½): {para_key} -> {str(structure_data[para_key])[:50]}...")
                    filled_count += 1

        # 5. å°†å›¾ç‰‡ä½œä¸ºé™„ä»¶é™„åŠ åˆ°æ–‡æ¡£æœ«å°¾
        if ordered_attachments:
            logger.info("ğŸ“ å¼€å§‹åœ¨æ–‡æ¡£æœ«å°¾é™„åŠ å›¾ç‰‡...")
            try:
                doc.add_page_break()
                doc.add_heading('é™„ä»¶åˆ—è¡¨', level=1)
                
                for i, (key, image_path) in enumerate(ordered_attachments):
                    attachment_counter = i + 1
                    if not image_path or not isinstance(image_path, str) or not os.path.exists(image_path):
                        logger.warning(f"âš ï¸ å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•ˆï¼Œè·³è¿‡é™„ä»¶ '{key}': {image_path}")
                        continue
                    
                    try:
                        heading_text = f"é™„ä»¶ {attachment_counter}: {key}"
                        doc.add_heading(heading_text, level=2)
                        doc.add_picture(image_path, width=Inches(6.0))
                        doc.add_paragraph()
                        logger.info(f"   âœ… æˆåŠŸé™„åŠ å›¾ç‰‡: {heading_text} ({image_path})")
                    except Exception as pic_e:
                        logger.error(f"âŒ é™„åŠ å›¾ç‰‡ '{key}' ({image_path}) æ—¶å‡ºé”™: {pic_e}")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†é™„ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        
        doc.save(output_path)
        logger.info(f"âœ… æ··åˆæ¨¡å¼å¡«å……å®Œæˆï¼Œå…±å¡«å…… {filled_count} ä¸ªå­—æ®µ: {output_path}")
        
        return {
            "sections_count": filled_count,
            "file_size": os.path.getsize(output_path),
            "validation": {"is_valid": True, "paragraph_count": len(doc.paragraphs), "table_count": len(doc.tables)}
        }

    def process_template_insertion(self, template_path: str, original_content: str, output_path: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ¨¡æ¿æ’å…¥å¤„ç†æµç¨‹ - ä¿æŒåŸå§‹æ¨¡æ¿ç»“æ„
        
        Args:
            template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„ (.doc æˆ– .docx)
            original_content: åŸå§‹å†…å®¹ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ¨¡æ¿æ’å…¥å¤„ç†...")
        
        try:
            # Stage 0: Convert .doc to .docx if necessary
            if template_path.lower().endswith('.doc'):
                logger.info(f"ğŸ“„ æ£€æµ‹åˆ°.docæ¨¡æ¿ï¼Œå¼€å§‹è½¬æ¢: {template_path}")
                extractor = DocumentExtractor()
                original_docx_path = extractor.convert_doc_to_docx(template_path)
            else:
                original_docx_path = template_path

            # Stage 0.5: é¢„å¤„ç†æ¨¡æ¿ï¼Œæå–å ä½ç¬¦
            processed_template_path = original_docx_path.replace(".docx", "_processed.docx")
            placeholders = self._preprocess_template_and_extract_placeholders(
                doc_path=original_docx_path,
                output_path=processed_template_path
            )
            
            # Stage 1: åˆ†æå¤„ç†åçš„æ¨¡æ¿ç»“æ„
            template_structure = self.analyze_template_structure(processed_template_path)

            # Stage 2: åˆ›å»ºè¾“å…¥æ•°æ®ç»“æ„
            input_data = {"original_content": original_content}

            # Stage 2.5: æ··åˆæ¨¡å¼AIæ˜ å°„
            fill_data = self.ai_generate_fill_data(
                template_structure=template_structure,
                placeholders=placeholders,
                input_data=input_data
            )
            
            # Stage 3: æ··åˆæ¨¡å¼å¡«å……ï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰
            generation_info = self.fill_template_with_ai_data(
                template_path=processed_template_path,
                output_path=output_path,
                fill_data=fill_data
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if original_docx_path != template_path and os.path.exists(original_docx_path):
                os.remove(original_docx_path)
            if os.path.exists(processed_template_path):
                os.remove(processed_template_path)
            
            logger.info(f"âœ… æ¨¡æ¿æ’å…¥å¤„ç†å®Œæˆ: {output_path}")
            return generation_info
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿æ’å…¥å¤„ç†å¤±è´¥: {e}", exc_info=True)
            raise

    def ai_generate_fill_data(self, template_structure: Dict[str, str], placeholders: List[str], input_data: Dict[str, Any]) -> Dict[str, str]:
        """
        æ··åˆæ¨¡å¼ - ä½¿ç”¨AIåŒæ—¶å¤„ç†æ¨¡æ¿ç»“æ„åŒ¹é…å’Œå ä½ç¬¦åŒ¹é…
        """
        logger.info("ğŸ§  å¼€å§‹æ··åˆæ¨¡å¼AIå­—æ®µæ˜ å°„...")
        
        try:
            # æ„å»ºæ··åˆæç¤º
            prompt = get_fill_data_prompt(
                json.dumps(template_structure, ensure_ascii=False, indent=2),
                json.dumps(placeholders, ensure_ascii=False, indent=2),
                json.dumps(input_data, ensure_ascii=False, indent=2)
            )
            
            logger.info("ğŸ§  æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ··åˆæ¨¡å¼æ˜ å°„...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            
            if not response or not response.choices or not response.choices[0].message.content:
                raise ValueError("AIå“åº”æ— æ•ˆæˆ–ä¸ºç©º")

            json_string = self._extract_json_from_response(response.choices[0].message.content)
            fill_data = json.loads(json_string)
            
            logger.info(f"âœ… AIæˆåŠŸç”Ÿæˆ {len(fill_data)} ä¸ªå­—æ®µçš„æ˜ å°„:")
            for key, value in fill_data.items():
                preview = str(value)[:70] + "..." if len(str(value)) > 70 else str(value)
                logger.info(f"   ğŸ”— {key} -> '{preview}'")
            
            return fill_data
            
        except Exception as e:
            logger.error(f"âŒ AIå­—æ®µæ˜ å°„é”™è¯¯: {e}", exc_info=True)
            return {}

    def _extract_json_from_response(self, response_content: str) -> str:
        """ä»AIå“åº”ä¸­æå–JSONå†…å®¹"""
        if "```json" in response_content:
            start = response_content.find("```json") + 7
            end = response_content.find("```", start)
            if end != -1:
                return response_content[start:end].strip()
            else:
                return response_content[response_content.find("```json") + 7:].strip()
        elif response_content.startswith("{") and response_content.endswith("}"):
            return response_content
        else:
            # æŸ¥æ‰¾JSONå¯¹è±¡
            start_idx = response_content.find("{")
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(response_content[start_idx:], start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            return response_content[start_idx:i+1]
            return response_content

# ====================================
# æ ¸å¿ƒ Function Call æ¥å£
# ====================================

def merge_template_with_context(
    template_json: Dict[str, str],
    original_content: str,
    api_key: str,
    output_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    AIæ™ºèƒ½åˆå¹¶æ¨¡æ¿ä¸ä¸Šä¸‹æ–‡å†…å®¹çš„æ ¸å¿ƒfunction callæ¥å£ - ç°åœ¨ä½¿ç”¨çœŸæ­£çš„æ¨¡æ¿æ’å…¥é€»è¾‘
    
    Args:
        template_json: æ¨¡æ¿JSONå­—å…¸ï¼Œé”®ä¸ºç« èŠ‚åï¼Œå€¼ä¸ºç« èŠ‚æè¿°  
        original_content: åŸå§‹æ–‡æ¡£å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        api_key: AI APIå¯†é’¥
        output_path: å¯é€‰çš„è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        DictåŒ…å«:
        - merged_json: åˆå¹¶åçš„å¡«å……ç»“æœ  
        - missing_fields: ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„å­—æ®µåˆ—è¡¨
        - output_path: æœ€ç»ˆæ–‡æ¡£è·¯å¾„
        - status: çŠ¶æ€ï¼ˆsuccess/partial/errorï¼‰
        - metadata: å…¶ä»–å…ƒæ•°æ®
    """
    logger.info("ğŸš€ å¼€å§‹çœŸæ­£çš„æ¨¡æ¿æ’å…¥å¤„ç†ï¼ˆFunction Callæ¨¡å¼ï¼‰...")
    
    try:
        # è¿™æ˜¯æ—§çš„æ¥å£ï¼Œç°åœ¨æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨template_jsonä½œä¸ºæ¨¡æ¿
        # å› ä¸ºæˆ‘ä»¬éœ€è¦ä¸€ä¸ªçœŸå®çš„Wordæ¨¡æ¿æ–‡ä»¶ï¼Œè€Œä¸æ˜¯JSONæè¿°
        # æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å›é€€åˆ°ä½¿ç”¨ContentMergeræ¥ç”Ÿæˆå†…å®¹ï¼Œç„¶ååˆ›å»ºæ–°æ–‡æ¡£
        logger.warning("âš ï¸ æ³¨æ„ï¼šæ­¤æ¥å£å°†ç”Ÿæˆæ–°æ–‡æ¡£ï¼Œä¸æ˜¯æ¨¡æ¿æ’å…¥ã€‚è¯·ä½¿ç”¨run_template_insertion()è¿›è¡ŒçœŸæ­£çš„æ¨¡æ¿æ’å…¥ã€‚")
        
        # 1. åˆå§‹åŒ–ç»„ä»¶
        merger = ContentMerger(api_key)
        
        # 2. AIæ™ºèƒ½åˆå¹¶
        logger.info("ğŸ§  ç¬¬ä¸€æ­¥ï¼šAIæ™ºèƒ½å†…å®¹åˆå¹¶")
        merged_json = merger.merge_content(template_json, original_content)
        
        # 3. åˆ†æç¼ºå¤±å­—æ®µ
        logger.info("ğŸ“Š ç¬¬äºŒæ­¥ï¼šåˆ†æå†…å®¹å®Œæ•´æ€§")
        missing_fields = merger.analyze_missing_fields(merged_json)
        
        # 4. ç”Ÿæˆè¾“å‡ºè·¯å¾„
        if not output_path:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_filename = f"merged_document_{timestamp}.docx"
            output_path = os.path.join(OUTPUT_DIR, output_filename)
        
        # 5. ç”Ÿæˆç®€å•çš„docxæ–‡æ¡£ï¼ˆä¸æ˜¯æ¨¡æ¿æ’å…¥ï¼‰
        logger.info("ğŸ“„ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç®€å•DOCXæ–‡æ¡£")
        doc = Document()
        doc.add_heading('AIæ™ºèƒ½åˆå¹¶æ–‡æ¡£', 0)
        
        for section_title, section_content in merged_json.items():
            doc.add_heading(section_title, level=1)
            doc.add_paragraph(str(section_content))
        
        doc.save(output_path)
        
        # 6. ç¡®å®šçŠ¶æ€
        if len(missing_fields) == 0:
            status = "success"
            message = "å†…å®¹åˆå¹¶å®Œæˆï¼Œæ‰€æœ‰å­—æ®µå¡«å……å®Œæ•´"
        elif len(missing_fields) < len(template_json) / 2:
            status = "partial"
            message = f"å†…å®¹åˆå¹¶éƒ¨åˆ†å®Œæˆï¼Œ{len(missing_fields)} ä¸ªå­—æ®µéœ€è¦è¡¥å……"
        else:
            status = "needs_more_info"
            message = f"å†…å®¹åˆå¹¶éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œ{len(missing_fields)} ä¸ªå­—æ®µç¼ºå¤±"
        
        logger.info(f"âœ… å†…å®¹åˆå¹¶å¤„ç†å®Œæˆï¼ŒçŠ¶æ€: {status}")
        
        return {
            "merged_json": merged_json,
            "missing_fields": missing_fields,
            "output_path": output_path,
            "status": status,
            "message": message,
            "metadata": {
                "template_sections": len(template_json),
                "filled_sections": len(template_json) - len(missing_fields),
                "completion_rate": (len(template_json) - len(missing_fields)) / len(template_json),
                "generation_info": {"file_size": os.path.getsize(output_path) if os.path.exists(output_path) else 0},
                "original_content_length": len(original_content),
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except ProcessingError as e:
        logger.error(f"âŒ å¤„ç†é”™è¯¯: {e}")
        return {
            "merged_json": {},
            "missing_fields": list(template_json.keys()),
            "output_path": None,
            "status": "error",
            "message": e.message,
            "metadata": {
                "error_code": e.error_code,
                "error_details": str(e),
                "timestamp": datetime.now().isoformat()
            }
        }
    except Exception as e:
        logger.error(f"âŒ æœªé¢„æœŸé”™è¯¯: {e}")
        return {
            "merged_json": {},
            "missing_fields": list(template_json.keys()),
            "output_path": None,
            "status": "error",
            "message": f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            "metadata": {
                "error_code": "UNEXPECTED_ERROR",
                "error_details": traceback.format_exc(),
                "timestamp": datetime.now().isoformat()
            }
        }

def template_insertion_with_context(
    template_file_path: str,
    original_content: str,
    api_key: str,
    output_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    çœŸæ­£çš„æ¨¡æ¿æ’å…¥åŠŸèƒ½ - ä¿æŒåŸå§‹æ¨¡æ¿ç»“æ„çš„function callæ¥å£
    
    Args:
        template_file_path: Wordæ¨¡æ¿æ–‡ä»¶è·¯å¾„ (.doc æˆ– .docx)
        original_content: åŸå§‹æ–‡æ¡£å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        api_key: AI APIå¯†é’¥
        output_path: å¯é€‰çš„è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        DictåŒ…å«å¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
    """
    logger.info("ğŸš€ å¼€å§‹çœŸæ­£çš„æ¨¡æ¿æ’å…¥å¤„ç†...")
    
    try:
        # 1. ç”Ÿæˆè¾“å‡ºè·¯å¾„
        if not output_path:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            template_name = os.path.splitext(os.path.basename(template_file_path))[0]
            output_filename = f"template_inserted_{template_name}_{timestamp}.docx"
            output_path = os.path.join(OUTPUT_DIR, output_filename)
        
        # 2. åˆå§‹åŒ–æ¨¡æ¿æ’å…¥å™¨
        inserter = TemplateInserter(api_key)
        
        # 3. æ‰§è¡Œæ¨¡æ¿æ’å…¥å¤„ç†
        generation_info = inserter.process_template_insertion(
            template_path=template_file_path,
            original_content=original_content,
            output_path=output_path
        )
        
        logger.info(f"âœ… çœŸæ­£çš„æ¨¡æ¿æ’å…¥å¤„ç†å®Œæˆ: {output_path}")
        
        return {
            "merged_json": {},  # è¿™é‡Œä¸è¿”å›merged_jsonï¼Œå› ä¸ºæ˜¯ç»“æ„åŒ–å¡«å……
            "missing_fields": [],  # è¿™é‡Œç®€åŒ–å¤„ç†
            "output_path": output_path,
            "status": "success",
            "message": "æ¨¡æ¿æ’å…¥å®Œæˆï¼Œä¿æŒäº†åŸå§‹æ¨¡æ¿ç»“æ„",
            "metadata": {
                "template_file": template_file_path,
                "filled_fields": generation_info.get("sections_count", 0),
                "generation_info": generation_info,
                "original_content_length": len(original_content),
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡æ¿æ’å…¥å¤„ç†å¤±è´¥: {e}")
        return {
            "merged_json": {},
            "missing_fields": [],
            "output_path": None,
            "status": "error",
            "message": f"æ¨¡æ¿æ’å…¥å¤±è´¥: {str(e)}",
            "metadata": {
                "template_file": template_file_path,
                "error_details": traceback.format_exc(),
                "timestamp": datetime.now().isoformat()
            }
        }

def extract_content_from_file(file_path: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶ä¸­æå–å†…å®¹çš„function callæ¥å£
    
    Args:
        file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
    Returns:
        DictåŒ…å«:
        - content: æå–çš„æ–‡æœ¬å†…å®¹
        - status: çŠ¶æ€ï¼ˆsuccess/errorï¼‰
        - metadata: æ–‡ä»¶ä¿¡æ¯ç­‰å…ƒæ•°æ®
    """
    logger.info(f"ğŸ“„ å¼€å§‹æå–æ–‡ä»¶å†…å®¹: {file_path}")
    
    try:
        extractor = DocumentExtractor()
        content = extractor.extract_from_file_path(file_path)
        
        return {
            "content": content,
            "status": "success",
            "message": "æ–‡æ¡£å†…å®¹æå–æˆåŠŸ",
            "metadata": {
                "file_path": file_path,
                "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                "content_length": len(content),
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except ProcessingError as e:
        logger.error(f"âŒ æ–‡æ¡£æå–é”™è¯¯: {e}")
        return {
            "content": "",
            "status": "error",
            "message": e.message,
            "metadata": {
                "file_path": file_path,
                "error_code": e.error_code,
                "error_details": str(e),
                "timestamp": datetime.now().isoformat()
            }
        }
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£æå–æœªé¢„æœŸé”™è¯¯: {e}")
        return {
            "content": "",
            "status": "error",
            "message": f"æ–‡æ¡£æå–å¤±è´¥: {str(e)}",
            "metadata": {
                "file_path": file_path,
                "error_code": "FILE_EXTRACTION_ERROR",
                "error_details": traceback.format_exc(),
                "timestamp": datetime.now().isoformat()
            }
        }

# ====================================
# å‘åå…¼å®¹æ¥å£ï¼ˆä¾›åŸæœ‰ä»£ç è°ƒç”¨ï¼‰
# ====================================

def run_template_insertion(template_json_input: Union[str, Dict[str, str]], original_file_path: str) -> str:
    """
    çœŸæ­£çš„æ¨¡æ¿æ’å…¥æ¥å£ - ç°åœ¨ä½¿ç”¨template_insertion_with_contextè¿›è¡ŒçœŸæ­£çš„æ¨¡æ¿æ’å…¥
    ï¼ˆä¿æŒå‘åå…¼å®¹çš„å‡½æ•°ç­¾åï¼‰
    
    Args:
        template_json_input: è¿™ä¸ªå‚æ•°ç°åœ¨è¢«å¿½ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥ä½¿ç”¨original_file_pathä½œä¸ºæ¨¡æ¿
        original_file_path: ç°åœ¨ä½œä¸ºWordæ¨¡æ¿æ–‡ä»¶è·¯å¾„ä½¿ç”¨
        
    Returns:
        ç”Ÿæˆçš„æ–‡æ¡£è·¯å¾„
    """
    logger.info("ğŸš€ çœŸæ­£çš„æ¨¡æ¿æ’å…¥æ¨¡å¼ï¼šå¯åŠ¨æ¨¡æ¿æ’å…¥å¤„ç†...")
    
    try:
        # åœ¨æ–°çš„é€»è¾‘ä¸­ï¼Œoriginal_file_path å®é™…ä¸Šæ˜¯æ¨¡æ¿æ–‡ä»¶
        # æˆ‘ä»¬éœ€è¦ä¸€äº›å®é™…çš„å†…å®¹æ¥å¡«å……æ¨¡æ¿
        template_file_path = original_file_path
        
        # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„åŸå§‹å†…å®¹ï¼Œå¦‚æœæ˜¯ä»JSONæä¾›çš„è¯
        if isinstance(template_json_input, str) and os.path.exists(template_json_input):
            with open(template_json_input, 'r', encoding='utf-8') as f:
                template_json = json.load(f)
            
            # å°†JSONå†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ä½œä¸ºåŸå§‹å†…å®¹  
            original_content = "åŸºäºJSONæ•°æ®çš„æ¨¡æ¿å¡«å……ï¼š\n\n"
            for key, value in template_json.items():
                original_content += f"{key}: {value}\n"
                
        elif isinstance(template_json_input, dict):
            # ç›´æ¥ä»å­—å…¸åˆ›å»ºå†…å®¹
            original_content = "åŸºäºJSONæ•°æ®çš„æ¨¡æ¿å¡«å……ï¼š\n\n"
            for key, value in template_json_input.items():
                original_content += f"{key}: {value}\n"
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æœ‰æ•ˆçš„JSONï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤å†…å®¹
            original_content = """
            è¿™æ˜¯ä¸€ä¸ªå»ºç­‘æ–½å·¥é¡¹ç›®çš„ç›¸å…³æ–‡æ¡£ã€‚
            é¡¹ç›®åç§°ï¼šæŸå¤§å‹å•†ä¸šå»ºç­‘æ–½å·¥é¡¹ç›®
            é¡¹ç›®åœ°ç‚¹ï¼šåŒ—äº¬å¸‚æœé˜³åŒº
            
            å®‰å…¨æ–¹é¢éœ€è¦ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
            1. é«˜ç©ºä½œä¸šå®‰å…¨æªæ–½
            2. æ–½å·¥ç°åœºå›´æŒ¡è®¾ç½®
            3. å®‰å…¨æ•™è‚²åŸ¹è®­
            
            è´¨é‡ç®¡ç†è¦æ±‚ï¼š
            - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡å›¾çº¸æ–½å·¥
            - ææ–™è¿›åœºéªŒæ”¶
            - å·¥åºè´¨é‡æ£€æŸ¥
            
            é¡¹ç›®è´Ÿè´£äººï¼šå¼ å·¥ç¨‹å¸ˆ
            å®¡æ ¸äººå‘˜ï¼šæç›‘ç†
            """
        
        # Get API key and use real template insertion
        api_key = get_api_key()
        insertion_result = template_insertion_with_context(
            template_file_path=template_file_path,
            original_content=original_content,
            api_key=api_key
        )
        
        if insertion_result["status"] == "error":
            raise ProcessingError(
                insertion_result["message"],
                insertion_result["metadata"].get("error_code", "INSERTION_ERROR"),
                500
            )
        
        final_doc_path = insertion_result["output_path"]
        logger.info(f"âœ… çœŸæ­£çš„æ¨¡æ¿æ’å…¥å¤„ç†å®Œæˆ: {final_doc_path}")
        
        return final_doc_path

    except (ProcessingError, FileNotFoundError) as e:
        logger.error(f"âŒ æ¨¡æ¿æ’å…¥å¤„ç†å¤±è´¥: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ æ¨¡æ¿æ’å…¥æœªé¢„æœŸé”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        raise ProcessingError(f"An unexpected error occurred: {str(e)}", "UNEXPECTED_ERROR", 500)

# ====================================
# æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•°
# ====================================

def test_merge_function():
    """æµ‹è¯•åˆå¹¶åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ¨¡æ¿åˆå¹¶åŠŸèƒ½")
    print("=" * 50)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_template = {
        "é¡¹ç›®æ¦‚è¿°": "é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯å’ŒèƒŒæ™¯ä»‹ç»",
        "å®‰å…¨æªæ–½": "æ–½å·¥å®‰å…¨ç®¡ç†æªæ–½å’Œè¦æ±‚",
        "è´¨é‡æ§åˆ¶": "å·¥ç¨‹è´¨é‡æ§åˆ¶æ ‡å‡†å’Œæ–¹æ³•"
    }
    
    test_content = """
    è¿™æ˜¯ä¸€ä¸ªå»ºç­‘æ–½å·¥é¡¹ç›®çš„ç›¸å…³æ–‡æ¡£ã€‚
    é¡¹ç›®åç§°ï¼šæŸå¤§å‹å•†ä¸šå»ºç­‘æ–½å·¥é¡¹ç›®
    é¡¹ç›®åœ°ç‚¹ï¼šåŒ—äº¬å¸‚æœé˜³åŒº
    
    å®‰å…¨æ–¹é¢éœ€è¦ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
    1. é«˜ç©ºä½œä¸šå®‰å…¨æªæ–½
    2. æ–½å·¥ç°åœºå›´æŒ¡è®¾ç½®
    3. å®‰å…¨æ•™è‚²åŸ¹è®­
    
    è´¨é‡ç®¡ç†è¦æ±‚ï¼š
    - ä¸¥æ ¼æŒ‰ç…§è®¾è®¡å›¾çº¸æ–½å·¥
    - ææ–™è¿›åœºéªŒæ”¶
    - å·¥åºè´¨é‡æ£€æŸ¥
    """
    
    try:
        # æµ‹è¯•APIå¯†é’¥è·å–
        api_key = get_api_key()
        print(f"âœ… APIå¯†é’¥è·å–æˆåŠŸ")
        
        # æµ‹è¯•åˆå¹¶åŠŸèƒ½
        result = merge_template_with_context(
            template_json=test_template,
            original_content=test_content,
            api_key=api_key
        )
        
        print(f"ğŸ“Š åˆå¹¶ç»“æœ:")
        print(f"   çŠ¶æ€: {result['status']}")
        print(f"   æ¶ˆæ¯: {result['message']}")
        print(f"   è¾“å‡ºæ–‡æ¡£: {result['output_path']}")
        print(f"   ç¼ºå¤±å­—æ®µ: {result['missing_fields']}")
        print(f"   å®Œæˆç‡: {result['metadata']['completion_rate']:.2%}")
        
        if result['output_path'] and os.path.exists(result['output_path']):
            print(f"âœ… æ–‡æ¡£ç”ŸæˆæˆåŠŸ: {result['output_path']}")
        else:
            print(f"âš ï¸ æ–‡æ¡£ç”Ÿæˆå¯èƒ½æœ‰é—®é¢˜")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_merge_function()