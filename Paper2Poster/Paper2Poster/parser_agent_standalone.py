#!/usr/bin/env python3
"""
ç‹¬ç«‹è§£ææ™ºèƒ½ä½“æ¥å£
ç”¨äºæµ‹è¯•Paper2Posterçš„è§£æåŠŸèƒ½
"""

import argparse
import json
import os
import random
import re
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
from camel.models import ModelFactory
from camel.agents import ChatAgent
from tenacity import retry, stop_after_attempt
from docling_core.types.doc import ImageRefMode, PictureItem, TableItem
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from jinja2 import Template
import PIL
import torch

# å°è¯•å¯¼å…¥markerï¼ˆå¯é€‰ï¼‰
try:
    from marker.models import create_model_dict
    from utils.src.model_utils import parse_pdf
    MARKER_AVAILABLE = True
except ImportError:
    MARKER_AVAILABLE = False
    print("Warning: Marker not available, will only use Docling for parsing")

# å°è¯•å¯¼å…¥CAMELå·¥å…·å‡½æ•°
try:
    from utils.wei_utils import get_agent_config, account_token
    from utils.src.utils import get_json_from_response
    CAMEL_UTILS_AVAILABLE = True
except ImportError:
    CAMEL_UTILS_AVAILABLE = False
    print("Warning: CAMEL utils not available, using simplified token counting")

load_dotenv()

# é…ç½®å¸¸é‡
IMAGE_RESOLUTION_SCALE = 5.0

# åˆå§‹åŒ–Doclingè½¬æ¢å™¨
pipeline_options = PdfPipelineOptions()
pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
pipeline_options.generate_page_images = True
pipeline_options.generate_picture_images = True

doc_converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)

class ParserAgent:
    """ç‹¬ç«‹çš„è§£ææ™ºèƒ½ä½“ç±»"""
    
    def __init__(self, model_name: str = "4o"):
        """
        åˆå§‹åŒ–è§£ææ™ºèƒ½ä½“
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º"4o"
        """
        self.model_name = model_name
        self.actor_config = self._get_agent_config(model_name)
        self.actor_model = self._create_model()
        self.actor_agent = self._create_agent()
        
    def _get_agent_config(self, model_name: str) -> Dict:
        """è·å–æ™ºèƒ½ä½“é…ç½®"""
        if CAMEL_UTILS_AVAILABLE:
            return get_agent_config(model_name)
        else:
            # ç®€åŒ–é…ç½®
            return {
                "model_type": "gpt-4o",
                "model_config": {},
                "model_platform": "openai"
            }
    
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
        try:
            if self.model_name.startswith('vllm_qwen'):
                return ModelFactory.create(
                    model_platform=self.actor_config['model_platform'],
                    model_type=self.actor_config['model_type'],
                    model_config_dict=self.actor_config['model_config'],
                    url=self.actor_config['url'],
                )
            else:
                return ModelFactory.create(
                    model_platform=self.actor_config['model_platform'],
                    model_type=self.actor_config['model_type'],
                    model_config_dict=self.actor_config['model_config'],
                )
        except Exception as e:
            print(f"Error creating model: {e}")
            return None
    
    def _create_agent(self) -> ChatAgent:
        """åˆ›å»ºèŠå¤©æ™ºèƒ½ä½“"""
        actor_sys_msg = 'You are the author of the paper, and you will create a poster for the paper.'
        
        return ChatAgent(
            system_message=actor_sys_msg,
            model=self.actor_model,
            message_window_size=10,
            token_limit=self.actor_config.get('token_limit', None)
        )
    
    def _account_token(self, response) -> Tuple[int, int]:
        """è®¡ç®—tokenä½¿ç”¨é‡"""
        if CAMEL_UTILS_AVAILABLE:
            return account_token(response)
        else:
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œè¿”å›ä¼°è®¡å€¼
            return len(str(response)), len(str(response))
    
    def _get_json_from_response(self, content: str) -> Dict:
        """ä»å“åº”ä¸­æå–JSON"""
        if CAMEL_UTILS_AVAILABLE:
            return get_json_from_response(content)
        else:
            # ç®€åŒ–ç‰ˆæœ¬
            try:
                # å°è¯•ç›´æ¥è§£æJSON
                return json.loads(content)
            except:
                # å°è¯•æå–JSONä»£ç å—
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(1))
                else:
                    # å°è¯•æå–{}åŒ…å›´çš„å†…å®¹
                    brace_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if brace_match:
                        return json.loads(brace_match.group(0))
                    else:
                        raise ValueError("Could not extract JSON from response")

    @retry(stop=stop_after_attempt(5))
    def parse_raw(self, pdf_path: str, output_dir: str = "parser_output") -> Tuple[Dict, Dict, Dict]:
        """
        è§£æPDFæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[Dict, Dict, Dict]: (content_json, images, tables)
        """
        print(f"å¼€å§‹è§£æPDF: {pdf_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨Doclingè§£æPDF
        print("ä½¿ç”¨Doclingè§£æPDF...")
        raw_result = doc_converter.convert(pdf_path)
        raw_markdown = raw_result.document.export_to_markdown()
        
        # æ¸…ç†markdownå†…å®¹
        markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
        text_content = markdown_clean_pattern.sub("", raw_markdown)
        
        # æ£€æŸ¥è§£æç»“æœ
        if len(text_content) < 500:
            print("Doclingè§£æç»“æœè¿‡çŸ­ï¼Œå°è¯•ä½¿ç”¨Marker...")
            if MARKER_AVAILABLE:
                parser_model = create_model_dict(device='cuda', dtype=torch.float16)
                text_content, rendered = parse_pdf(pdf_path, model_lst=parser_model, save_file=False)
            else:
                print("Warning: Markerä¸å¯ç”¨ï¼Œç»§ç»­ä½¿ç”¨Doclingç»“æœ")
        
        print(f"è§£æå¾—åˆ°æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹
        print("ä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹...")
        content_json = self._reorganize_content_with_llm(text_content)
        
        # ç¬¬ä¸‰æ­¥ï¼šæå–å›¾ç‰‡å’Œè¡¨æ ¼
        print("æå–å›¾ç‰‡å’Œè¡¨æ ¼...")
        images, tables = self._extract_images_and_tables(raw_result, output_dir)
        
        # ä¿å­˜ç»“æœ
        self._save_results(content_json, images, tables, output_dir)
        
        return content_json, images, tables
    
    def _reorganize_content_with_llm(self, text_content: str) -> Dict:
        """ä½¿ç”¨LLMé‡æ–°ç»„ç»‡å†…å®¹"""
        # åŠ è½½æç¤ºæ¨¡æ¿
        template_content = self._get_prompt_template()
        template = Template(template_content)
        
        # ç”Ÿæˆæç¤º
        prompt = template.render(markdown_document=text_content)
        
        # è°ƒç”¨LLM
        while True:
            self.actor_agent.reset()
            response = self.actor_agent.step(prompt)
            input_token, output_token = self._account_token(response)
            
            print(f"LLMè°ƒç”¨å®Œæˆ - è¾“å…¥tokens: {input_token}, è¾“å‡ºtokens: {output_token}")
            
            try:
                content_json = self._get_json_from_response(response.msgs[0].content)
                
                if len(content_json) > 0:
                    # éªŒè¯å’Œä¼˜åŒ–ç»“æœ
                    content_json = self._validate_and_optimize_content(content_json)
                    return content_json
                else:
                    print("LLMè¿”å›ç©ºç»“æœï¼Œé‡è¯•...")
                    
            except Exception as e:
                print(f"è§£æLLMå“åº”å¤±è´¥: {e}")
                print("é‡è¯•...")
                
                # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œæˆªæ–­é‡è¯•
                if self.model_name.startswith('vllm_qwen'):
                    text_content = text_content[:80000]
                    prompt = template.render(markdown_document=text_content)
    
    def _get_prompt_template(self) -> str:
        """è·å–æç¤ºæ¨¡æ¿"""
        return """You are a document content divider and extractor specialist, expert in dividing and extracting content from various types of documents and reorganizing it into a two-level json format for later poster generation.

Based on given markdown document, generate a JSON output for later poster generation, make sure the output is concise and focused.

Step-by-Step Instructions:
1. Identify Sections and Subsections in document and identify sections and subsections based on the heading levels and logical structure.

2. Divide Content: Reorganize the content into sections and subsections, ensuring that each subsection contains approximately 500 words.

3. Refine Titles: Create titles for each section with at most 3 words.

4. Remove Unwanted Elements: Eliminate any unwanted elements such as headers, footers, text surrounded by `~~` indicating deletion.

5. Refine Text: For content, you should keep as much raw text as possible. Do not include citations.

6. Length: you should control the length of each section, according to their importance according to your understanding of the paper. For important sections, their content should be long.

7. Make sure there is a poster title section at the beginning, and it should contain information like paper title, author, organization etc.

8. The "meta" key contains the meta information of the poster, where the title should be the raw title of the paper and is not summarized.

9. There **must** be a section for the poster title.

Example Output:
{
    "meta": {
        "poster_title": "raw title of the paper",
        "authors": "authors of the paper",
        "affiliations": "affiliations of the authors"
    },
    "sections": [
        {
            "title": "Poster Title & Author",
            "content": "content of poster title and author"
        },
        {
            "title": "title of section1",
            "content": "content of section 1"
        },
        {
            "title": "title of section2",
            "content": "content of section 2"
        }
    ]
}

Give your output in JSON format
Input:
{{ markdown_document }}
Output:"""
    
    def _validate_and_optimize_content(self, content_json: Dict) -> Dict:
        """éªŒè¯å’Œä¼˜åŒ–å†…å®¹"""
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if 'sections' not in content_json:
            raise ValueError("Missing 'sections' field in response")
        
        # éªŒè¯æ¯ä¸ªsectionçš„æ ¼å¼
        has_title = False
        for section in content_json['sections']:
            if not isinstance(section, dict) or 'title' not in section or 'content' not in section:
                raise ValueError("Invalid section format")
            if 'title' in section['title'].lower():
                has_title = True
        
        if not has_title:
            raise ValueError("Missing title section")
        
        # å¦‚æœsectionè¿‡å¤šï¼Œè¿›è¡Œæ™ºèƒ½é€‰æ‹©
        if len(content_json['sections']) > 9:
            print(f"Sectionæ•°é‡è¿‡å¤š({len(content_json['sections'])}ä¸ª)ï¼Œè¿›è¡Œæ™ºèƒ½é€‰æ‹©...")
            selected_sections = (
                content_json['sections'][:2] + 
                random.sample(content_json['sections'][2:-2], 5) + 
                content_json['sections'][-2:]
            )
            content_json['sections'] = selected_sections
            print(f"é€‰æ‹©åå‰©ä½™{len(content_json['sections'])}ä¸ªsections")
        
        return content_json
    
    def _extract_images_and_tables(self, raw_result, output_dir: str) -> Tuple[Dict, Dict]:
        """æå–å›¾ç‰‡å’Œè¡¨æ ¼"""
        images = {}
        tables = {}
        
        # æå–è¡¨æ ¼
        table_counter = 0
        for element, _level in raw_result.document.iterate_items():
            if isinstance(element, TableItem):
                table_counter += 1
                caption = element.caption_text(raw_result.document)
                
                if len(caption) > 0:
                    table_img_path = os.path.join(output_dir, f"table-{table_counter}.png")
                    
                    # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                    with open(table_img_path, "wb") as fp:
                        element.get_image(raw_result.document).save(fp, "PNG")
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    table_img = PIL.Image.open(table_img_path)
                    tables[str(table_counter)] = {
                        'caption': caption,
                        'table_path': table_img_path,
                        'width': table_img.width,
                        'height': table_img.height,
                        'figure_size': table_img.width * table_img.height,
                        'figure_aspect': table_img.width / table_img.height,
                    }
        
        # æå–å›¾ç‰‡
        picture_counter = 0
        for element, _level in raw_result.document.iterate_items():
            if isinstance(element, PictureItem):
                picture_counter += 1
                caption = element.caption_text(raw_result.document)
                
                if len(caption) > 0:
                    image_img_path = os.path.join(output_dir, f"picture-{picture_counter}.png")
                    
                    # ä¿å­˜å›¾ç‰‡
                    with open(image_img_path, "wb") as fp:
                        element.get_image(raw_result.document).save(fp, "PNG")
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    image_img = PIL.Image.open(image_img_path)
                    images[str(picture_counter)] = {
                        'caption': caption,
                        'image_path': image_img_path,
                        'width': image_img.width,
                        'height': image_img.height,
                        'figure_size': image_img.width * image_img.height,
                        'figure_aspect': image_img.width / image_img.height,
                    }
        
        print(f"æå–äº† {len(tables)} ä¸ªè¡¨æ ¼å’Œ {len(images)} ä¸ªå›¾ç‰‡")
        return images, tables
    
    def _save_results(self, content_json: Dict, images: Dict, tables: Dict, output_dir: str):
        """ä¿å­˜è§£æç»“æœ"""
        # ä¿å­˜ç»“æ„åŒ–å†…å®¹
        content_path = os.path.join(output_dir, "parsed_content.json")
        with open(content_path, 'w', encoding='utf-8') as f:
            json.dump(content_json, f, indent=4, ensure_ascii=False)
        print(f"ç»“æ„åŒ–å†…å®¹å·²ä¿å­˜åˆ°: {content_path}")
        
        # ä¿å­˜å›¾ç‰‡ä¿¡æ¯
        images_path = os.path.join(output_dir, "images.json")
        with open(images_path, 'w', encoding='utf-8') as f:
            json.dump(images, f, indent=4, ensure_ascii=False)
        print(f"å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {images_path}")
        
        # ä¿å­˜è¡¨æ ¼ä¿¡æ¯
        tables_path = os.path.join(output_dir, "tables.json")
        with open(tables_path, 'w', encoding='utf-8') as f:
            json.dump(tables, f, indent=4, ensure_ascii=False)
        print(f"è¡¨æ ¼ä¿¡æ¯å·²ä¿å­˜åˆ°: {tables_path}")
        
        # ä¿å­˜æ±‡æ€»ä¿¡æ¯
        summary = {
            "total_sections": len(content_json.get("sections", [])),
            "total_images": len(images),
            "total_tables": len(tables),
            "meta_info": content_json.get("meta", {}),
            "section_titles": [section.get("title", "") for section in content_json.get("sections", [])]
        }
        
        summary_path = os.path.join(output_dir, "summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=4, ensure_ascii=False)
        print(f"æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°: {summary_path}")
    
    def get_parsing_stats(self, content_json: Dict, images: Dict, tables: Dict) -> Dict:
        """è·å–è§£æç»Ÿè®¡ä¿¡æ¯"""
        total_text_length = sum(len(section.get("content", "")) for section in content_json.get("sections", []))
        
        stats = {
            "sections_count": len(content_json.get("sections", [])),
            "total_text_length": total_text_length,
            "avg_section_length": total_text_length / max(len(content_json.get("sections", [])), 1),
            "images_count": len(images),
            "tables_count": len(tables),
            "total_figures": len(images) + len(tables),
            "has_meta": "meta" in content_json,
            "has_title_section": any("title" in section.get("title", "").lower() for section in content_json.get("sections", []))
        }
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹è§£ææ™ºèƒ½ä½“æµ‹è¯•æ¥å£")
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", default="parser_output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model", default="4o", help="ä½¿ç”¨çš„æ¨¡å‹åç§°")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.pdf_path):
        print(f"é”™è¯¯: PDFæ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_path}")
        return
    
    try:
        # åˆ›å»ºè§£ææ™ºèƒ½ä½“
        print(f"åˆå§‹åŒ–è§£ææ™ºèƒ½ä½“ï¼Œä½¿ç”¨æ¨¡å‹: {args.model}")
        parser_agent = ParserAgent(model_name=args.model)
        
        # æ‰§è¡Œè§£æ
        content_json, images, tables = parser_agent.parse_raw(args.pdf_path, args.output_dir)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = parser_agent.get_parsing_stats(content_json, images, tables)
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*50)
        print("è§£æå®Œæˆï¼")
        print("="*50)
        
        print(f"ğŸ“„ è§£æç»Ÿè®¡:")
        print(f"  - Sectionsæ•°é‡: {stats['sections_count']}")
        print(f"  - æ€»æ–‡æœ¬é•¿åº¦: {stats['total_text_length']:,} å­—ç¬¦")
        print(f"  - å¹³å‡sectioné•¿åº¦: {stats['avg_section_length']:.0f} å­—ç¬¦")
        print(f"  - å›¾ç‰‡æ•°é‡: {stats['images_count']}")
        print(f"  - è¡¨æ ¼æ•°é‡: {stats['tables_count']}")
        print(f"  - æ€»å›¾è¡¨æ•°é‡: {stats['total_figures']}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  - ç»“æ„åŒ–å†…å®¹: {args.output_dir}/parsed_content.json")
        print(f"  - å›¾ç‰‡ä¿¡æ¯: {args.output_dir}/images.json")
        print(f"  - è¡¨æ ¼ä¿¡æ¯: {args.output_dir}/tables.json")
        print(f"  - æ±‡æ€»ä¿¡æ¯: {args.output_dir}/summary.json")
        
        if args.verbose:
            print(f"\nğŸ“‹ Sectionæ ‡é¢˜:")
            for i, title in enumerate(stats['section_titles'], 1):
                print(f"  {i}. {title}")
            
            if content_json.get("meta"):
                print(f"\nğŸ“ è®ºæ–‡ä¿¡æ¯:")
                meta = content_json["meta"]
                print(f"  - æ ‡é¢˜: {meta.get('poster_title', 'N/A')}")
                print(f"  - ä½œè€…: {meta.get('authors', 'N/A')}")
                print(f"  - æœºæ„: {meta.get('affiliations', 'N/A')}")
        
        print(f"\nâœ… è§£ææˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        import traceback
        if args.verbose:
            traceback.print_exc()


if __name__ == "__main__":
    main() 